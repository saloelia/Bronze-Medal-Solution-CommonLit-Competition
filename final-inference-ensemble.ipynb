{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be34ba6b",
   "metadata": {
    "papermill": {
     "duration": 0.006036,
     "end_time": "2023-10-11T16:10:15.518697",
     "exception": false,
     "start_time": "2023-10-11T16:10:15.512661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1ac588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:10:15.532848Z",
     "iopub.status.busy": "2023-10-11T16:10:15.532419Z",
     "iopub.status.idle": "2023-10-11T16:10:48.800224Z",
     "shell.execute_reply": "2023-10-11T16:10:48.799204Z"
    },
    "papermill": {
     "duration": 33.278219,
     "end_time": "2023-10-11T16:10:48.802358",
     "exception": false,
     "start_time": "2023-10-11T16:10:15.524139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\r\n",
      "Installing collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.7.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\" #pyspell\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/input/textstat-install-mit/package\") ##Textstat\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce9874",
   "metadata": {
    "papermill": {
     "duration": 0.005742,
     "end_time": "2023-10-11T16:10:48.813929",
     "exception": false,
     "start_time": "2023-10-11T16:10:48.808187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7724e706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:10:48.826173Z",
     "iopub.status.busy": "2023-10-11T16:10:48.825903Z",
     "iopub.status.idle": "2023-10-11T16:11:05.237431Z",
     "shell.execute_reply": "2023-10-11T16:11:05.236659Z"
    },
    "papermill": {
     "duration": 16.420093,
     "end_time": "2023-10-11T16:11:05.239484",
     "exception": false,
     "start_time": "2023-10-11T16:10:48.819391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, disable_progress_bar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import ne_chunk, word_tokenize, pos_tag\n",
    "# logging setting \n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "disable_progress_bar()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61dd779",
   "metadata": {
    "papermill": {
     "duration": 0.00561,
     "end_time": "2023-10-11T16:11:05.251085",
     "exception": false,
     "start_time": "2023-10-11T16:11:05.245475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Seed to 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf90843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:11:05.263223Z",
     "iopub.status.busy": "2023-10-11T16:11:05.263037Z",
     "iopub.status.idle": "2023-10-11T16:11:05.271870Z",
     "shell.execute_reply": "2023-10-11T16:11:05.271139Z"
    },
    "papermill": {
     "duration": 0.016837,
     "end_time": "2023-10-11T16:11:05.273531",
     "exception": false,
     "start_time": "2023-10-11T16:11:05.256694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c75f44",
   "metadata": {
    "papermill": {
     "duration": 0.005334,
     "end_time": "2023-10-11T16:11:05.284415",
     "exception": false,
     "start_time": "2023-10-11T16:11:05.279081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d150d69e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:11:05.296787Z",
     "iopub.status.busy": "2023-10-11T16:11:05.296492Z",
     "iopub.status.idle": "2023-10-11T16:11:05.302099Z",
     "shell.execute_reply": "2023-10-11T16:11:05.301377Z"
    },
    "papermill": {
     "duration": 0.013658,
     "end_time": "2023-10-11T16:11:05.303762",
     "exception": false,
     "start_time": "2023-10-11T16:11:05.290104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb5913",
   "metadata": {
    "papermill": {
     "duration": 0.005654,
     "end_time": "2023-10-11T16:11:05.314975",
     "exception": false,
     "start_time": "2023-10-11T16:11:05.309321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2af6d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:11:05.327293Z",
     "iopub.status.busy": "2023-10-11T16:11:05.326936Z",
     "iopub.status.idle": "2023-10-11T16:11:05.428438Z",
     "shell.execute_reply": "2023-10-11T16:11:05.427660Z"
    },
    "papermill": {
     "duration": 0.109773,
     "end_time": "2023-10-11T16:11:05.430321",
     "exception": false,
     "start_time": "2023-10-11T16:11:05.320548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n",
    "\n",
    "prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n",
    "prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n",
    "summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n",
    "summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n",
    "sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c2436",
   "metadata": {
    "papermill": {
     "duration": 0.005544,
     "end_time": "2023-10-11T16:11:05.441768",
     "exception": false,
     "start_time": "2023-10-11T16:11:05.436224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Pre-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc47a48a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:11:05.455059Z",
     "iopub.status.busy": "2023-10-11T16:11:05.454884Z",
     "iopub.status.idle": "2023-10-11T16:11:07.134619Z",
     "shell.execute_reply": "2023-10-11T16:11:07.133762Z"
    },
    "papermill": {
     "duration": 1.688875,
     "end_time": "2023-10-11T16:11:07.136731",
     "exception": false,
     "start_time": "2023-10-11T16:11:05.447856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, \n",
    "                model_name: str,\n",
    "                ) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n",
    "        self.STOP_WORDS = set(stopwords.words('english'))\n",
    "        \n",
    "        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n",
    "        self.speller = SpellChecker() #Speller(lang='en')\n",
    "        \n",
    "    def count_text_length(self, df: pd.DataFrame, col:str) -> pd.Series:\n",
    "        \"\"\" text length \"\"\"\n",
    "        tokenizer=self.tokenizer\n",
    "        return df[col].progress_apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "    #JUST STOP WORDS OVERLAP\n",
    "    def word_overlap_count(self, row):\n",
    "        \"\"\" intersection(prompt_text, text) \"\"\"        \n",
    "        def check_is_stop_word(word):\n",
    "            return word in self.STOP_WORDS\n",
    "        \n",
    "#         prompt_words = row['prompt_tokens']\n",
    "#         summary_words = row['summary_tokens']\n",
    "                \n",
    "        prompt_words = list(self.spacy_ner_model.tokenizer(row['prompt_text']))\n",
    "        summary_words = list(self.spacy_ner_model.tokenizer(row['text']))\n",
    "        \n",
    "        prompt_words = [str(word) for word in prompt_words]\n",
    "        summary_words = [str(word) for word in summary_words]\n",
    "    \n",
    "        if self.STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "            \n",
    "    #REAL OVERLAP WORD\n",
    "    def word_overlap_count_real(self, row):\n",
    "        \"\"\" intersection(prompt_text, text) \"\"\"        \n",
    "        def check_is_stop_word_real(word):\n",
    "            return word not in self.STOP_WORDS\n",
    "        \n",
    "        prompt_words = list(self.spacy_ner_model.tokenizer(row['prompt_text']))\n",
    "        summary_words = list(self.spacy_ner_model.tokenizer(row['text']))\n",
    "        \n",
    "        prompt_words = [str(word) for word in prompt_words]\n",
    "        summary_words = [str(word) for word in summary_words]\n",
    "        \n",
    "        if self.STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word_real, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word_real, summary_words))\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "    \n",
    "    \n",
    "    def ngrams(self, token, n):\n",
    "        # Use the zip function to help us generate n-grams\n",
    "        # Concatentate the tokens into ngrams and return\n",
    "        ngrams = zip(*[token[i:] for i in range(n)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    def ngram_co_occurrence(self, row, n: int):\n",
    "        # Tokenize the original text and summary into words\n",
    "        original_tokens = row['prompt_tokens']\n",
    "        summary_tokens = row['summary_tokens']\n",
    "\n",
    "        # Generate n-grams for the original text and summary\n",
    "        original_ngrams = set(self.ngrams(original_tokens, n))\n",
    "        summary_ngrams = set(self.ngrams(summary_tokens, n))\n",
    "\n",
    "        # Calculate the number of common n-grams\n",
    "        common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "\n",
    "        # # Optionally, you can get the frequency of common n-grams for a more nuanced analysis\n",
    "        # original_ngram_freq = Counter(ngrams(original_words, n))\n",
    "        # summary_ngram_freq = Counter(ngrams(summary_words, n))\n",
    "        # common_ngram_freq = {ngram: min(original_ngram_freq[ngram], summary_ngram_freq[ngram]) for ngram in common_ngrams}\n",
    "\n",
    "        return len(common_ngrams)\n",
    "    \n",
    "    def ner_overlap_count(self, row, mode:str):\n",
    "        model = self.spacy_ner_model\n",
    "        def clean_ners(ner_list):\n",
    "            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n",
    "        prompt = model(row['prompt_text'])\n",
    "        summary = model(row['text'])\n",
    "\n",
    "        if \"spacy\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n",
    "        elif \"stanza\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.type) for token in summary.ents])\n",
    "        else:\n",
    "            raise Exception(\"Model not supported\")\n",
    "\n",
    "        prompt_ner = clean_ners(prompt_ner)\n",
    "        summary_ner = clean_ners(summary_ner)\n",
    "\n",
    "        intersecting_ners = prompt_ner.intersection(summary_ner)\n",
    "        \n",
    "        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            return ner_dict\n",
    "        elif mode == \"test\":\n",
    "            return {key: ner_dict.get(key) for key in self.ner_keys}\n",
    "\n",
    "    \n",
    "    def quotes_count(self, row):\n",
    "        summary = row['text']\n",
    "        text = row['prompt_text']\n",
    "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "        if len(quotes_from_summary)>0:\n",
    "            return [quote in text for quote in quotes_from_summary].count(True)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def spelling(self, text):\n",
    "        \n",
    "#         wordlist=text.split()\n",
    "        wordlist = self.spacy_ner_model.tokenizer(text)\n",
    "        wordlist = [str(word) for word in wordlist]\n",
    "        amount_miss = len(list(self.speller.unknown(wordlist)))\n",
    "\n",
    "        return amount_miss\n",
    "    \n",
    "    \n",
    "    def calculate_pos_ratios(self,text):\n",
    "        pos_tags = pos_tag(nltk.word_tokenize(text))\n",
    "        pos_counts = Counter(tag for word, tag in pos_tags)\n",
    "        total_words = len(pos_tags)\n",
    "        ratios = {tag: count / total_words for tag, count in pos_counts.items()}\n",
    "        return ratios\n",
    "    \n",
    "    def calculate_sentiment_scores(self,text):\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        sentiment_scores = sid.polarity_scores(text)\n",
    "        return sentiment_scores\n",
    "    \n",
    "    def calculate_punctuation_ratios(self,text):\n",
    "        total_chars = len(text)\n",
    "        punctuation_counts = Counter(char for char in text if char in '.,!?;:\"()[]{}')\n",
    "        ratios = {char: count / total_chars for char, count in punctuation_counts.items()}\n",
    "        return ratios\n",
    "    \n",
    "    def calculate_keyword_density(self,row):\n",
    "        keywords = set(row['prompt_text'].split())\n",
    "        text_words = row['text'].split()\n",
    "        keyword_count = sum(1 for word in text_words if word in keywords)\n",
    "        return keyword_count / len(text_words)\n",
    "    \n",
    "    \n",
    "    def run(self,prompts: pd.DataFrame,summaries:pd.DataFrame,mode:str) -> pd.DataFrame:\n",
    "        \n",
    "        # before merge preprocess\n",
    "        \n",
    "#         prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n",
    "#             lambda x: len(self.tokenizer.encode(x))\n",
    "#         )\n",
    "        \n",
    "        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: len(list(self.spacy_ner_model.tokenizer(x)))\n",
    "        )\n",
    "        \n",
    "        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: self.tokenizer.convert_ids_to_tokens(\n",
    "                self.tokenizer.encode(x), \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "#         summaries[\"summary_length\"] = summaries[\"text\"].apply(\n",
    "#             lambda x: len(self.tokenizer.encode(x))\n",
    "#         )\n",
    "\n",
    "        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n",
    "            lambda x: len(list(self.spacy_ner_model.tokenizer(x)))\n",
    "        )\n",
    "    \n",
    "        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n",
    "            lambda x: self.tokenizer.convert_ids_to_tokens(\n",
    "                self.tokenizer.encode(x), \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "        )\n",
    "        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n",
    "\n",
    "        # merge prompts and summaries\n",
    "        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n",
    "\n",
    "        # after merge preprocess\n",
    "        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n",
    "        \n",
    "        #stop words overlap\n",
    "        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n",
    "        \n",
    "        \n",
    "        input_df['bigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence,args=(2,), axis=1 \n",
    "        )\n",
    "        input_df['trigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence, args=(3,), axis=1\n",
    "        )\n",
    "        \n",
    "#         # Crate dataframe with count of each category NERs overlap for all the summaries\n",
    "#         # Because it spends too much time for this feature, I don't use this time.\n",
    "#         ners_count_df  = input_df.progress_apply(\n",
    "#             lambda row: pd.Series(self.ner_overlap_count(row, mode=mode), dtype='float64'), axis=1\n",
    "#         ).fillna(0)\n",
    "#         self.ner_keys = ners_count_df.columns\n",
    "#         ners_count_df['sum'] = ners_count_df.sum(axis=1)\n",
    "#         ners_count_df.columns = ['NER_' + col for col in ners_count_df.columns]\n",
    "#         # join ner count dataframe with train dataframe\n",
    "#         input_df = pd.concat([input_df, ners_count_df], axis=1)\n",
    "        \n",
    "        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n",
    "        \n",
    "        #Additional\n",
    "        \n",
    "        #real overlap words\n",
    "        input_df['real_word_overlap_count'] = input_df.progress_apply(self.word_overlap_count_real, axis=1)\n",
    "        \n",
    "        input_df['sentence_length'] = input_df['text'].progress_apply(lambda x: len(x.split('.')))\n",
    "        input_df['vocabulary_richness'] = input_df['text'].progress_apply(lambda x: len(set(x.split())))\n",
    "        input_df['avg_word_length'] = input_df['text'].progress_apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
    "        input_df['comma_count'] = input_df['text'].progress_apply(lambda x: x.count(','))\n",
    "        input_df['semicolon_count'] = input_df['text'].progress_apply(lambda x: x.count(';'))\n",
    "        \n",
    "        input_df['pos_ratios'] = input_df['text'].progress_apply(self.calculate_pos_ratios)\n",
    "        input_df['pos_mean'] = input_df['pos_ratios'].progress_apply(lambda x: np.mean(list(x.values())))\n",
    "        \n",
    "        input_df['sentiment_scores'] = input_df['text'].progress_apply(self.calculate_sentiment_scores)\n",
    "        \n",
    "        sentiment_columns = pd.DataFrame(list(input_df['sentiment_scores']))\n",
    "        input_df = pd.concat([input_df, sentiment_columns], axis=1)\n",
    "        \n",
    "        input_df['exclamation_count'] = input_df['text'].progress_apply(lambda x: x.count('!'))\n",
    "        input_df['question_count'] = input_df['text'].progress_apply(lambda x: x.count('?'))\n",
    "        input_df['quote_count'] = input_df['text'].progress_apply(lambda x: x.count('\"'))\n",
    "        \n",
    "        input_df['punctuation_ratios'] = input_df['text'].progress_apply(self.calculate_punctuation_ratios)\n",
    "        input_df['punctuation_sum'] = input_df['punctuation_ratios'].progress_apply(lambda x: np.sum(list(x.values())))\n",
    "        \n",
    "        input_df['keyword_density'] = input_df.progress_apply(self.calculate_keyword_density, axis=1)\n",
    "\n",
    "        input_df['sentiment_scores_prompt'] = input_df['prompt_text'].progress_apply(self.calculate_sentiment_scores)\n",
    "\n",
    "        sentiment_columns_prompt = pd.DataFrame(list(input_df['sentiment_scores_prompt']))\n",
    "        sentiment_columns_prompt.columns = [col +'_prompt' for col in sentiment_columns_prompt.columns]\n",
    "        \n",
    "        input_df = pd.concat([input_df, sentiment_columns_prompt], axis=1)\n",
    "        \n",
    "        input_df['jaccard_similarity'] = input_df.progress_apply(lambda row: len(set(word_tokenize(row['prompt_text'])) & set(word_tokenize(row['text']))) / len(set(word_tokenize(row['prompt_text'])) | set(word_tokenize(row['text']))), axis=1)\n",
    "        \n",
    "        \n",
    "        ###########TEXTSTAT FEARURES#############\n",
    "        input_df['flesch_reading_ease'] = input_df['text'].progress_apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "        input_df['flesch_kincaid_grade'] = input_df['text'].progress_apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "        input_df['gunning_fog'] = input_df['text'].progress_apply(lambda x: textstat.gunning_fog(x))\n",
    "        #input_df['smog_index'] = input_df['text'].progress_apply(lambda x: textstat.smog_index(x))\n",
    "        input_df['automated_readability_index'] = input_df['text'].progress_apply(lambda x: textstat.automated_readability_index(x))\n",
    "        input_df['coleman_liau_index'] = input_df['text'].progress_apply(lambda x: textstat.coleman_liau_index(x))\n",
    "        input_df['linsear_write_formula'] = input_df['text'].progress_apply(lambda x: textstat.linsear_write_formula(x))\n",
    "        input_df['dale_chall_readability_score'] = input_df['text'].progress_apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "        input_df['text_standard'] = input_df['text'].progress_apply(lambda x: textstat.text_standard(x,float_output=True))\n",
    "        input_df['spache_readability'] = input_df['text'].progress_apply(lambda x: textstat.spache_readability(x))\n",
    "        input_df['mcalpine_eflaw'] = input_df['text'].progress_apply(lambda x: textstat.mcalpine_eflaw(x))\n",
    "        input_df['reading_time'] = input_df['text'].progress_apply(lambda x: textstat.reading_time(x))\n",
    "        input_df['syllable_count'] = input_df['text'].progress_apply(lambda x: textstat.syllable_count(x))\n",
    "        input_df['polysyllabcount'] = input_df['text'].progress_apply(lambda x: textstat.polysyllabcount(x))\n",
    "        input_df['monosyllabcount'] = input_df['text'].progress_apply(lambda x: textstat.monosyllabcount(x))\n",
    "      \n",
    "        \n",
    "        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\",\"pos_ratios\",\"sentiment_scores\",\"punctuation_ratios\",\"sentiment_scores_prompt\"])\n",
    "    \n",
    "preprocessor = Preprocessor(model_name=\"debertav3base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa5e98",
   "metadata": {
    "papermill": {
     "duration": 0.005797,
     "end_time": "2023-10-11T16:11:07.148591",
     "exception": false,
     "start_time": "2023-10-11T16:11:07.142794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create Test NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786e1474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:11:07.160974Z",
     "iopub.status.busy": "2023-10-11T16:11:07.160781Z",
     "iopub.status.idle": "2023-10-11T16:11:08.470156Z",
     "shell.execute_reply": "2023-10-11T16:11:08.469511Z"
    },
    "papermill": {
     "duration": 1.34319,
     "end_time": "2023-10-11T16:11:08.497370",
     "exception": false,
     "start_time": "2023-10-11T16:11:07.154180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 2594.28it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2097.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3919.91it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3980.36it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3494.53it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 905.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 6864.65it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 9505.50it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 7781.64it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10356.31it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4687.68it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 22.92it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 5862.06it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 66.89it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 7667.83it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 8089.30it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 8460.52it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 6870.28it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 6878.73it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3953.16it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 77.66it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1277.00it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3958.76it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 7332.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 270.53it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 5905.39it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 6455.26it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 9898.06it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 9177.91it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 7806.99it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 8603.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 9372.75it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 9430.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10699.76it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10261.29it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11530.73it/s]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/common-lit-train/commont-lit-train.csv\")\n",
    "gkf = GroupKFold(n_splits=4)\n",
    "\n",
    "for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n",
    "    train.loc[val_index, \"fold\"] = i\n",
    "\n",
    "test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "615bf658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:11:08.537535Z",
     "iopub.status.busy": "2023-10-11T16:11:08.537302Z",
     "iopub.status.idle": "2023-10-11T16:11:08.567318Z",
     "shell.execute_reply": "2023-10-11T16:11:08.566653Z"
    },
    "papermill": {
     "duration": 0.051911,
     "end_time": "2023-10-11T16:11:08.569648",
     "exception": false,
     "start_time": "2023-10-11T16:11:08.517737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>length_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>text_standard</th>\n",
       "      <th>spache_readability</th>\n",
       "      <th>mcalpine_eflaw</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>polysyllabcount</th>\n",
       "      <th>monosyllabcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>1.5</td>\n",
       "      <td>19.58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>1.5</td>\n",
       "      <td>19.58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>1.5</td>\n",
       "      <td>19.58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>1.5</td>\n",
       "      <td>19.58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text  summary_length  splling_err_num  \\\n",
       "0  000000ffffff    abc123  Example text 1               3                0   \n",
       "1  111111eeeeee    def789  Example text 2               3                0   \n",
       "2  222222cccccc    abc123  Example text 3               3                0   \n",
       "3  333333dddddd    def789  Example text 4               3                0   \n",
       "\n",
       "  prompt_question     prompt_title       prompt_text  prompt_length  \\\n",
       "0    Summarize...  Example Title 1  Heading\\nText...              4   \n",
       "1    Summarize...  Example Title 2  Heading\\nText...              4   \n",
       "2    Summarize...  Example Title 1  Heading\\nText...              4   \n",
       "3    Summarize...  Example Title 2  Heading\\nText...              4   \n",
       "\n",
       "   length_ratio  ...  coleman_liau_index  linsear_write_formula  \\\n",
       "0          0.75  ...               -2.38                    1.5   \n",
       "1          0.75  ...               -2.38                    1.5   \n",
       "2          0.75  ...               -2.38                    1.5   \n",
       "3          0.75  ...               -2.38                    1.5   \n",
       "\n",
       "   dale_chall_readability_score  text_standard  spache_readability  \\\n",
       "0                         19.58            6.0                4.13   \n",
       "1                         19.58            6.0                4.13   \n",
       "2                         19.58            6.0                4.13   \n",
       "3                         19.58            6.0                4.13   \n",
       "\n",
       "   mcalpine_eflaw  reading_time  syllable_count  polysyllabcount  \\\n",
       "0             4.0          0.18               5                1   \n",
       "1             4.0          0.18               5                1   \n",
       "2             4.0          0.18               5                1   \n",
       "3             4.0          0.18               5                1   \n",
       "\n",
       "   monosyllabcount  \n",
       "0                2  \n",
       "1                2  \n",
       "2                2  \n",
       "3                2  \n",
       "\n",
       "[4 rows x 49 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9166688",
   "metadata": {
    "papermill": {
     "duration": 0.017397,
     "end_time": "2023-10-11T16:11:08.602237",
     "exception": false,
     "start_time": "2023-10-11T16:11:08.584840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Regressor - Predict Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91656c53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:11:08.649795Z",
     "iopub.status.busy": "2023-10-11T16:11:08.649560Z",
     "iopub.status.idle": "2023-10-11T16:11:08.665798Z",
     "shell.execute_reply": "2023-10-11T16:11:08.665106Z"
    },
    "papermill": {
     "duration": 0.04595,
     "end_time": "2023-10-11T16:11:08.667818",
     "exception": false,
     "start_time": "2023-10-11T16:11:08.621868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScoreRegressor:\n",
    "    def __init__(self, \n",
    "                model_dir: str,\n",
    "                inputs: List[str],\n",
    "                target_cols: List[str],\n",
    "                max_length: int,\n",
    "                ):\n",
    "        \n",
    "        self.input_col = \"input\" # col name of model input after text concat sep token\n",
    "        self.input_text_cols = inputs\n",
    "        self.target_cols = target_cols\n",
    "        self.model_dir = model_dir\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        checkpoint = os.listdir(self.model_dir)[0]\n",
    "        path_to_checkpoint = os.path.join(self.model_dir,checkpoint)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path_to_checkpoint)\n",
    "        self.data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "\n",
    "    def concatenate_with_sep_token(self, row):\n",
    "        sep = \" \" + self.tokenizer.sep_token + \" \"        \n",
    "        return sep.join(row[self.input_text_cols])\n",
    "\n",
    "    def tokenize_function(self, examples: pd.DataFrame):\n",
    "        labels = [examples[\"content\"], examples[\"wording\"]]\n",
    "        tokenized = self.tokenizer(examples[self.input_col],\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=self.max_length)\n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    def tokenize_function_test(self, examples: pd.DataFrame):\n",
    "        tokenized = self.tokenizer(examples[self.input_col],\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=self.max_length)\n",
    "        return tokenized\n",
    "        \n",
    "\n",
    "        \n",
    "    def predict(self, \n",
    "                test_df: pd.DataFrame,\n",
    "                batch_size: int,\n",
    "                fold: int,\n",
    "               ):\n",
    "        \"\"\"predict content score\"\"\"\n",
    "        \n",
    "        test_df[self.input_col] = test_df.apply(self.concatenate_with_sep_token, axis=1)\n",
    "\n",
    "        test_dataset = Dataset.from_pandas(test_df[[self.input_col]], preserve_index=False) \n",
    "        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n",
    "        \n",
    "        checkpoint = os.listdir(self.model_dir)[0]\n",
    "        path_to_checkpoint = os.path.join(self.model_dir,checkpoint)\n",
    "        \n",
    "        model_config = AutoConfig.from_pretrained(path_to_checkpoint)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path_to_checkpoint,config=model_config)\n",
    "        model.eval()\n",
    "        test_args = TrainingArguments(\n",
    "            output_dir='/kaggle/working/',\n",
    "            do_train=False,\n",
    "            do_predict=True,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            dataloader_drop_last=False,\n",
    "            fp16=True,\n",
    "            auto_find_batch_size=True,\n",
    "        )\n",
    "\n",
    "        # init trainer\n",
    "        infer_content = Trainer(\n",
    "                      model = model, \n",
    "                      tokenizer=self.tokenizer,\n",
    "                      data_collator=self.data_collator,\n",
    "                      args = test_args)\n",
    "\n",
    "        preds = infer_content.predict(test_tokenized_dataset)[0]\n",
    "        pred_df = pd.DataFrame(\n",
    "            preds, \n",
    "            columns=[\n",
    "                f\"content_pred\", \n",
    "                f\"wording_pred\"\n",
    "           ]\n",
    "        )\n",
    "        \n",
    "        model.cpu()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2e032",
   "metadata": {
    "papermill": {
     "duration": 0.019472,
     "end_time": "2023-10-11T16:11:08.707099",
     "exception": false,
     "start_time": "2023-10-11T16:11:08.687627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8610ead2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:11:08.748248Z",
     "iopub.status.busy": "2023-10-11T16:11:08.747738Z",
     "iopub.status.idle": "2023-10-11T16:11:08.761771Z",
     "shell.execute_reply": "2023-10-11T16:11:08.761075Z"
    },
    "papermill": {
     "duration": 0.037171,
     "end_time": "2023-10-11T16:11:08.763988",
     "exception": false,
     "start_time": "2023-10-11T16:11:08.726817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(\n",
    "    test: pd.DataFrame,\n",
    "    mode: str,\n",
    "    targets:List[str],\n",
    "    inputs: List[str],\n",
    "    n_splits: int,\n",
    "    batch_size: int,\n",
    "    max_length : int,\n",
    "    path_to_folds: str,\n",
    "    model_col_num: int,\n",
    "    ):\n",
    "    \"\"\"predict using mean folds\"\"\"\n",
    "    \n",
    "    test_df = test.copy()\n",
    "    columns = list(test_df.columns.values)\n",
    "    \n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        model_dir =  f\"{path_to_folds}/fold_{fold}/{fold}\"\n",
    "\n",
    "        csr = ScoreRegressor(\n",
    "            model_dir = model_dir,\n",
    "            target_cols=targets,\n",
    "            inputs= inputs,\n",
    "            max_length=max_length,\n",
    "           )\n",
    "        \n",
    "        pred_df = csr.predict(\n",
    "            test_df=test_df, \n",
    "            batch_size=batch_size,\n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        \n",
    "        test_df[f\"content_{mode}_pred_{fold}\"] = pred_df[f\"content_pred\"].values\n",
    "        test_df[f\"wording_{mode}_pred_{fold}\"] = pred_df[f\"wording_pred\"].values\n",
    "\n",
    "    test_df[f\"content_{mode}_pred_{model_col_num}\"] = test_df[[f\"content_{mode}_pred_{fold}\" for fold in range(n_splits)]].mean(axis=1)\n",
    "    test_df[f\"wording_{mode}_pred_{model_col_num}\"] = test_df[[f\"wording_{mode}_pred_{fold}\" for fold in range(n_splits)]].mean(axis=1)\n",
    "    \n",
    "    return test_df[[f\"content_{mode}_pred_{model_col_num}\", f\"wording_{mode}_pred_{model_col_num}\"]]\n",
    "\n",
    "\n",
    "\n",
    "def validate(\n",
    "    train: pd.DataFrame,\n",
    "    mode: str,\n",
    "    targets: List[str],\n",
    "    inputs: List[str],\n",
    "    n_splits: int,\n",
    "    batch_size: int,\n",
    "    path_to_folds: str,\n",
    "    model_col_num: int,\n",
    "    max_length : int,\n",
    "    ):\n",
    "    \n",
    "    train_df = train.copy()\n",
    "    columns = list(train_df.columns.values)\n",
    "\n",
    "    for fold in range(n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "\n",
    "        valid_data = train_df[train_df[\"fold\"] == fold]\n",
    "        \n",
    "        model_dir =  f\"{path_to_folds}/fold_{fold}/{fold}\"\n",
    "        \n",
    "        csr = ScoreRegressor(\n",
    "            model_dir = model_dir,\n",
    "            target_cols=targets,\n",
    "            inputs= inputs,\n",
    "            max_length=max_length,\n",
    "           )\n",
    "\n",
    "        pred_df = csr.predict(\n",
    "            test_df=valid_data,\n",
    "            batch_size=batch_size,\n",
    "            fold=fold\n",
    "        )\n",
    "\n",
    "        train_df.loc[valid_data.index, f\"content_{mode}_pred_{model_col_num}\"] = pred_df[f\"content_pred\"].values\n",
    "        train_df.loc[valid_data.index, f\"wording_{mode}_pred_{model_col_num}\"] = pred_df[f\"wording_pred\"].values\n",
    "\n",
    "    return train_df[[f\"content_{mode}_pred_{model_col_num}\", f\"wording_{mode}_pred_{model_col_num}\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca23886",
   "metadata": {
    "papermill": {
     "duration": 0.067931,
     "end_time": "2023-10-11T16:11:08.851376",
     "exception": false,
     "start_time": "2023-10-11T16:11:08.783445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Models Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d94fb66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:11:08.892004Z",
     "iopub.status.busy": "2023-10-11T16:11:08.891739Z",
     "iopub.status.idle": "2023-10-11T16:11:08.897589Z",
     "shell.execute_reply": "2023-10-11T16:11:08.896866Z"
    },
    "papermill": {
     "duration": 0.030212,
     "end_time": "2023-10-11T16:11:08.901145",
     "exception": false,
     "start_time": "2023-10-11T16:11:08.870933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ALBERT_V2_PATH = \"/kaggle/input/albert-v3-base-exp1/content/albert-base-v2-model/exp_1\"\n",
    "ALL_MPNET_BASE_V2_PATH = \"/kaggle/input/all-mpnet-base-v2-exp1/exp_1/content/all-mpnet-base-v2-model/exp_1\"\n",
    "DEBERTA_V3_BASE_PATH = \"/kaggle/input/deberta-v3-base-exp1/exp_1/content/deberta-v3-base-model/exp_1\"\n",
    "# ELECTRA_BASE_DISCRIMINATOR_PATH = \"/kaggle/input/electra-base-discriminator-exp1/content/electra-base-discriminator/exp_1\"\n",
    "FUNNEL_MEDIUM_BASE_PATH = \"/kaggle/input/funnel-medium-base-exp1/exp_1/content/medium-base-model/exp_1\"\n",
    "# ROBERTA_BASE_SQUAD2_PATH = \"/kaggle/input/roberta-base-squad2-exp1/content/roberta-base-squad2/exp_1\"\n",
    "XLM_ROBERTA_BASE_PATH = \"/kaggle/input/xlm-roberta-base-exp1/exp_1/content/xlm-roberta-base-model/exp_1\"\n",
    "DEBERTA_V3_LARGE_PATH = \"/kaggle/input/deberta-v3-large-exp1/exp_1/content/deberta-v3-large-model/exp_1\"\n",
    "XLM_ROBERTA_LARGE_PATH = \"/kaggle/input/xlm-roberta-large-exp1/exp_1/content/xlm-roberta-large-model/exp_1\"\n",
    "\n",
    "\n",
    "model_paths = [ALL_MPNET_BASE_V2_PATH,FUNNEL_MEDIUM_BASE_PATH,XLM_ROBERTA_BASE_PATH,DEBERTA_V3_LARGE_PATH,XLM_ROBERTA_LARGE_PATH,DEBERTA_V3_BASE_PATH]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b23725",
   "metadata": {
    "papermill": {
     "duration": 0.01941,
     "end_time": "2023-10-11T16:11:08.940028",
     "exception": false,
     "start_time": "2023-10-11T16:11:08.920618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predict on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d1ad499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:11:08.979865Z",
     "iopub.status.busy": "2023-10-11T16:11:08.979652Z",
     "iopub.status.idle": "2023-10-11T16:44:29.310494Z",
     "shell.execute_reply": "2023-10-11T16:44:29.309596Z"
    },
    "papermill": {
     "duration": 2000.353235,
     "end_time": "2023-10-11T16:44:29.312660",
     "exception": false,
     "start_time": "2023-10-11T16:11:08.959425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/all-mpnet-base-v2-exp1/exp_1/content/all-mpnet-base-v2-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/funnel-medium-base-exp1/exp_1/content/medium-base-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlm-roberta-base-exp1/exp_1/content/xlm-roberta-base-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/deberta-v3-large-exp1/exp_1/content/deberta-v3-large-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlm-roberta-large-exp1/exp_1/content/xlm-roberta-large-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/deberta-v3-base-exp1/exp_1/content/deberta-v3-base-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets = [\"wording\", \"content\"]\n",
    "input_cols = [\"prompt_title\", \"prompt_question\", \"text\"]\n",
    "\n",
    "ensemble_df_train = pd.DataFrame({})\n",
    "for i,model_path in enumerate(model_paths): \n",
    "    print(model_path)\n",
    "    batch_size=12\n",
    "    if 'large' in model_path:\n",
    "        batch_size=10\n",
    "    train_exp = validate(\n",
    "        train,\n",
    "        mode=\"multi\",\n",
    "        targets=targets,\n",
    "        inputs=input_cols,\n",
    "        batch_size=batch_size,\n",
    "        n_splits=4,\n",
    "        max_length=512,\n",
    "        path_to_folds=model_path,\n",
    "        model_col_num=i,\n",
    "    )\n",
    "    ensemble_df_train = pd.concat([ensemble_df_train,train_exp],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582089a",
   "metadata": {
    "papermill": {
     "duration": 0.012871,
     "end_time": "2023-10-11T16:44:29.338911",
     "exception": false,
     "start_time": "2023-10-11T16:44:29.326040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predict on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e1282ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:44:29.366508Z",
     "iopub.status.busy": "2023-10-11T16:44:29.365819Z",
     "iopub.status.idle": "2023-10-11T16:49:02.377119Z",
     "shell.execute_reply": "2023-10-11T16:49:02.376221Z"
    },
    "papermill": {
     "duration": 273.027581,
     "end_time": "2023-10-11T16:49:02.379319",
     "exception": false,
     "start_time": "2023-10-11T16:44:29.351738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/all-mpnet-base-v2-exp1/exp_1/content/all-mpnet-base-v2-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/funnel-medium-base-exp1/exp_1/content/medium-base-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlm-roberta-base-exp1/exp_1/content/xlm-roberta-base-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/deberta-v3-large-exp1/exp_1/content/deberta-v3-large-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlm-roberta-large-exp1/exp_1/content/xlm-roberta-large-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/deberta-v3-base-exp1/exp_1/content/deberta-v3-base-model/exp_1\n",
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets = [\"wording\", \"content\"]\n",
    "input_cols = [\"prompt_title\", \"prompt_question\", \"text\"]\n",
    "\n",
    "ensemble_df = pd.DataFrame({})\n",
    "for i,model_path in enumerate(model_paths): \n",
    "    print(model_path)\n",
    "    batch_size=12\n",
    "    if 'large' in model_path:\n",
    "        batch_size=10\n",
    "    test_exp = predict(\n",
    "        test,\n",
    "        mode=\"multi\",\n",
    "        targets=targets,\n",
    "        inputs=input_cols,\n",
    "        batch_size=12,\n",
    "        n_splits=4,\n",
    "        max_length=512,\n",
    "        path_to_folds=model_path,\n",
    "        model_col_num=i,\n",
    "    )\n",
    "    ensemble_df = pd.concat([ensemble_df,test_exp],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f642e1e",
   "metadata": {
    "papermill": {
     "duration": 0.015558,
     "end_time": "2023-10-11T16:49:02.411674",
     "exception": false,
     "start_time": "2023-10-11T16:49:02.396116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Regular Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa1a96b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:49:02.444624Z",
     "iopub.status.busy": "2023-10-11T16:49:02.444161Z",
     "iopub.status.idle": "2023-10-11T16:49:03.261646Z",
     "shell.execute_reply": "2023-10-11T16:49:03.260788Z"
    },
    "papermill": {
     "duration": 0.836095,
     "end_time": "2023-10-11T16:49:03.263654",
     "exception": false,
     "start_time": "2023-10-11T16:49:02.427559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_ensemble_train_df = pd.DataFrame({})\n",
    "final_ensemble_train_df['pred_content'] = ensemble_df_train.filter(regex=(f'content')).apply(np.mean,axis=1)\n",
    "final_ensemble_train_df['pred_wording'] = ensemble_df_train.filter(regex=(f'wording')).apply(np.mean,axis=1)\n",
    "\n",
    "final_ensemble_df = pd.DataFrame({})\n",
    "final_ensemble_df['pred_content'] = ensemble_df.filter(regex=(f'content')).apply(np.mean,axis=1)\n",
    "final_ensemble_df['pred_wording'] = ensemble_df.filter(regex=(f'wording')).apply(np.mean,axis=1)\n",
    "\n",
    "train = pd.concat([train,final_ensemble_train_df],axis=1)\n",
    "test = pd.concat([test,final_ensemble_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff890505",
   "metadata": {
    "papermill": {
     "duration": 0.015438,
     "end_time": "2023-10-11T16:49:03.295197",
     "exception": false,
     "start_time": "2023-10-11T16:49:03.279759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a3672fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:49:03.327354Z",
     "iopub.status.busy": "2023-10-11T16:49:03.326721Z",
     "iopub.status.idle": "2023-10-11T16:49:03.331822Z",
     "shell.execute_reply": "2023-10-11T16:49:03.330926Z"
    },
    "papermill": {
     "duration": 0.022882,
     "end_time": "2023-10-11T16:49:03.333424",
     "exception": false,
     "start_time": "2023-10-11T16:49:03.310542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = [\"content\", \"wording\"]\n",
    "\n",
    "drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\",\n",
    "                \"prompt_question\", \"prompt_title\",\n",
    "                \"prompt_text\",\"prompt_length\",\n",
    "                \"avg_word_length\",\"semicolon_count\",\"neg\",\"neu\",\"pos\",\"compound\",\n",
    "                \"exclamation_count\",\"question_count\",\"punctuation_sum\",\"neg_prompt\",\"neu_prompt\",\"pos_prompt\",\n",
    "                \"compound_prompt\",\"flesch_reading_ease\",\"flesch_kincaid_grade\",\"gunning_fog\",\"automated_readability_index\",\n",
    "                \"coleman_liau_index\",\"linsear_write_formula\",\"dale_chall_readability_score\",\"text_standard\",\"spache_readability\",\n",
    "                \"mcalpine_eflaw\"\n",
    "               ] + targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68f5c07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:49:03.365502Z",
     "iopub.status.busy": "2023-10-11T16:49:03.365085Z",
     "iopub.status.idle": "2023-10-11T16:49:05.263871Z",
     "shell.execute_reply": "2023-10-11T16:49:05.262894Z"
    },
    "papermill": {
     "duration": 1.916638,
     "end_time": "2023-10-11T16:49:05.265595",
     "exception": false,
     "start_time": "2023-10-11T16:49:03.348957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3004\n",
      "[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.017606\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttrain's rmse: 0.395312\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2926\n",
      "[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.039959\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.47581\n",
      "Early stopping, best iteration is:\n",
      "[142]\ttrain's rmse: 0.475182\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2927\n",
      "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.013356\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttrain's rmse: 0.416279\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3022\n",
      "[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.044904\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.448204\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttrain's rmse: 0.448204\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3004\n",
      "[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.031791\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttrain's rmse: 0.516452\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2926\n",
      "[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.060941\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.643892\n",
      "Early stopping, best iteration is:\n",
      "[87]\ttrain's rmse: 0.641917\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2927\n",
      "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.028040\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.500712\n",
      "Early stopping, best iteration is:\n",
      "[83]\ttrain's rmse: 0.500573\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3022\n",
      "[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.168933\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's rmse: 0.619494\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttrain's rmse: 0.618566\n"
     ]
    }
   ],
   "source": [
    "model_dict = {}\n",
    "\n",
    "for target in targets:\n",
    "    models = []\n",
    "    \n",
    "    for fold in range(4):\n",
    "\n",
    "        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n",
    "        y_train_cv = train[train[\"fold\"] != fold][target]\n",
    "\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n",
    "\n",
    "        params = {\n",
    "                  'boosting_type': 'gbdt',\n",
    "                  'random_state': 42,\n",
    "                  'objective': 'regression',\n",
    "                  'metric': 'rmse',\n",
    "                  'learning_rate': 0.048,\n",
    "                  'lambda_l1': 0.0,\n",
    "                  'lambda_l2': 0.011\n",
    "                  }\n",
    "\n",
    "        evaluation_results = {}\n",
    "        model = lgb.train(params,\n",
    "                          num_boost_round=10000,\n",
    "                            #categorical_feature = categorical_features,\n",
    "                          valid_names=['train', 'valid'],\n",
    "                          train_set=dtrain,\n",
    "                          valid_sets=dval,\n",
    "                          callbacks=[\n",
    "                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n",
    "                               lgb.log_evaluation(100),\n",
    "                              lgb.callback.record_evaluation(evaluation_results)\n",
    "                            ],\n",
    "                          )\n",
    "        models.append(model)\n",
    "    \n",
    "    model_dict[target] = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17d038aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:49:05.299292Z",
     "iopub.status.busy": "2023-10-11T16:49:05.298779Z",
     "iopub.status.idle": "2023-10-11T16:49:05.413795Z",
     "shell.execute_reply": "2023-10-11T16:49:05.412925Z"
    },
    "papermill": {
     "duration": 0.133532,
     "end_time": "2023-10-11T16:49:05.415392",
     "exception": false,
     "start_time": "2023-10-11T16:49:05.281860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_rmse : 0.4328681846407399\n",
      "wording_rmse : 0.5664064483417809\n",
      "mcrmse : 0.4996373164912604\n"
     ]
    }
   ],
   "source": [
    "rmses = []\n",
    "\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    \n",
    "    for fold, model in enumerate(models):\n",
    "        # ilocで取り出す行を指定\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        pred = model.predict(X_eval_cv)\n",
    "\n",
    "        trues.extend(y_eval_cv)\n",
    "        preds.extend(pred)\n",
    "        \n",
    "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
    "    print(f\"{target}_rmse : {rmse}\")\n",
    "    rmses = rmses + [rmse]\n",
    "\n",
    "print(f\"mcrmse : {sum(rmses) / len(rmses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d4838",
   "metadata": {
    "papermill": {
     "duration": 0.015658,
     "end_time": "2023-10-11T16:49:05.447096",
     "exception": false,
     "start_time": "2023-10-11T16:49:05.431438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predict on Test and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0af76547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:49:05.480562Z",
     "iopub.status.busy": "2023-10-11T16:49:05.479881Z",
     "iopub.status.idle": "2023-10-11T16:49:05.484875Z",
     "shell.execute_reply": "2023-10-11T16:49:05.484158Z"
    },
    "papermill": {
     "duration": 0.023571,
     "end_time": "2023-10-11T16:49:05.486517",
     "exception": false,
     "start_time": "2023-10-11T16:49:05.462946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_columns = [\"student_id\", \"prompt_id\", \"text\",\n",
    "                \"prompt_question\", \"prompt_title\",\n",
    "                \"prompt_text\",\"prompt_length\",\n",
    "                \"avg_word_length\",\"semicolon_count\",\"neg\",\"neu\",\"pos\",\"compound\",\n",
    "                \"exclamation_count\",\"question_count\",\"punctuation_sum\",\"neg_prompt\",\"neu_prompt\",\"pos_prompt\",\n",
    "                \"compound_prompt\",\"flesch_reading_ease\",\"flesch_kincaid_grade\",\"gunning_fog\",\"automated_readability_index\",\n",
    "                \"coleman_liau_index\",\"linsear_write_formula\",\"dale_chall_readability_score\",\"text_standard\",\"spache_readability\",\n",
    "                \"mcalpine_eflaw\"\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a6a2cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:49:05.519815Z",
     "iopub.status.busy": "2023-10-11T16:49:05.519171Z",
     "iopub.status.idle": "2023-10-11T16:49:05.536732Z",
     "shell.execute_reply": "2023-10-11T16:49:05.535960Z"
    },
    "papermill": {
     "duration": 0.036033,
     "end_time": "2023-10-11T16:49:05.538289",
     "exception": false,
     "start_time": "2023-10-11T16:49:05.502256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "    preds = []\n",
    "\n",
    "    for fold, model in enumerate(models):\n",
    "        # ilocで取り出す行を指定\n",
    "        X_eval_cv = test.drop(columns=drop_columns)\n",
    "\n",
    "        pred = model.predict(X_eval_cv)\n",
    "        preds.append(pred)\n",
    "    \n",
    "    pred_dict[target] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eba9ed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:49:05.571490Z",
     "iopub.status.busy": "2023-10-11T16:49:05.570896Z",
     "iopub.status.idle": "2023-10-11T16:49:05.586865Z",
     "shell.execute_reply": "2023-10-11T16:49:05.586126Z"
    },
    "papermill": {
     "duration": 0.034848,
     "end_time": "2023-10-11T16:49:05.588886",
     "exception": false,
     "start_time": "2023-10-11T16:49:05.554038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    preds = pred_dict[target]\n",
    "    for i, pred in enumerate(preds):\n",
    "        test[f\"{target}_pred_{i}\"] = pred\n",
    "\n",
    "    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(4)]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4206294b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:49:05.623843Z",
     "iopub.status.busy": "2023-10-11T16:49:05.623665Z",
     "iopub.status.idle": "2023-10-11T16:49:05.631263Z",
     "shell.execute_reply": "2023-10-11T16:49:05.630560Z"
    },
    "papermill": {
     "duration": 0.026093,
     "end_time": "2023-10-11T16:49:05.632917",
     "exception": false,
     "start_time": "2023-10-11T16:49:05.606824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2337.254163,
   "end_time": "2023-10-11T16:49:09.895184",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-11T16:10:12.641021",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

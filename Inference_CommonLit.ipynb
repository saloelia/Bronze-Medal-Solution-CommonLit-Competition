{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvNK1Mx7D_Ze",
        "outputId": "a3760795-4f5b-42e6-9e4e-7468a6b17bb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "max_length=1024\n",
        "PATH_TO_FOLDER = f'/content/drive/MyDrive/CommonLit/deberta-v3-base/batch_{BATCH_SIZE}/{max_length}'"
      ],
      "metadata": {
        "id": "-nt1sbaDEBQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold,GroupKFold"
      ],
      "metadata": {
        "id": "HTuo3ZAFEI-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "AJQmNtKJELhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba52105-3d90-4ea8-d7c1-585afa68a57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c commonlit-evaluate-student-summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbT4u_vpIyCJ",
        "outputId": "6ff7985e-6688-4274-bbef-38ae5dd2edc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "commonlit-evaluate-student-summaries.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/commonlit-evaluate-student-summaries.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")"
      ],
      "metadata": {
        "id": "wis8yncPIszS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/summaries_train.csv')\n",
        "\n",
        "X = data['text'].to_list()\n",
        "y = data[['wording', 'content']].values"
      ],
      "metadata": {
        "id": "qpbfi46DESGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/CommonLit/nlp_features_csv.csv\")\n",
        "data.drop([7165,7166,7167,7168],0,inplace=True)\n",
        "selected_vars = ['content', 'wording', 'word_count', 'sentence_length', 'vocabulary_richness', 'avg_word_length',\n",
        "                   'comma_count', 'semicolon_count', 'exclamation_count', 'question_count',\n",
        "                   'quote_count', 'unique_word_count', 'pos_mean', 'compound', 'pos', 'neg', 'neu',\n",
        "                   'punctuation_sum', 'word_overlap', 'prompt_length', 'text_to_prompt_ratio', 'keyword_density'\n",
        "                  ,'compound_prompt', 'pos_prompt', 'neg_prompt', 'neu_prompt', 'jaccard_similarity']\n",
        "train = data[selected_vars]\n",
        "\n",
        "X = train.drop(['wording','content'],1)\n",
        "X = X.to_numpy()\n",
        "y = train[['wording', 'content']].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzbd1ZypQWiS",
        "outputId": "78a54b94-280a-461b-fec3-8758c1adf8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-4243817690ce>:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  data.drop([7165,7166,7167,7168],0,inplace=True)\n",
            "<ipython-input-19-4243817690ce>:10: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  X = train.drop(['wording','content'],1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_kfold = GroupKFold(n_splits=4)\n",
        "folds = {'39c16e':0,\n",
        "         '3b9047':1,\n",
        "         'ebad26':2,\n",
        "         '814d6b':3\n",
        "}\n",
        "groups = np.array([folds[data['prompt_id'][i]] for i in range(len(data))])\n",
        "dfs = []\n",
        "for i, (train_index, test_index) in tqdm(enumerate(group_kfold.split(X, y, groups))):\n",
        "    print(f'Begining Fold:{i+1}')\n",
        "\n",
        "    model_wording = lgb.LGBMRegressor(random_state=42)\n",
        "    model_content = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "    model_wording.fit(X[train_index],y[train_index][:,0])\n",
        "    word_prediction = model_wording.predict(X[test_index])\n",
        "\n",
        "\n",
        "    model_content.fit(X[train_index],y[train_index][:,1])\n",
        "    content_prediction = model_content.predict(X[test_index])\n",
        "\n",
        "    df = pd.DataFrame(index=test_index)\n",
        "    df['true_word'] = y[test_index][:,0]\n",
        "    df['true_content'] = y[test_index][:,1]\n",
        "    df['pred_word'] = word_prediction\n",
        "    df['pred_content'] = content_prediction\n",
        "    dfs.append(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64K6tDT_QCDw",
        "outputId": "2f3c6014-1247-4a6e-f7a3-c3cdcd513de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begining Fold:1\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000882 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3013\n",
            "[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 25\n",
            "[LightGBM] [Info] Start training from score -0.031791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r1it [00:00,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3013\n",
            "[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 25\n",
            "[LightGBM] [Info] Start training from score 0.017606\n",
            "Begining Fold:2\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001521 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3050\n",
            "[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score -0.060941\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001466 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3050\n",
            "[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score -0.039959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [00:00,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begining Fold:3\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002487 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3085\n",
            "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 25\n",
            "[LightGBM] [Info] Start training from score 0.028040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [00:01,  2.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3085\n",
            "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 25\n",
            "[LightGBM] [Info] Start training from score 0.013356\n",
            "Begining Fold:4\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001624 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3117\n",
            "[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score -0.168933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [00:01,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3117\n",
            "[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score -0.044904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_kfold = GroupKFold(n_splits=4)\n",
        "folds = {'39c16e':0,\n",
        "         '3b9047':1,\n",
        "         'ebad26':2,\n",
        "         '814d6b':3\n",
        "}\n",
        "groups = np.array([folds[data['prompt_id'][i]] for i in range(len(data))])\n",
        "dfs1 = []\n",
        "for i, (train_index, test_index) in tqdm(enumerate(group_kfold.split(X, y, groups))):\n",
        "    print(f'Begining Fold:{i+1}')\n",
        "\n",
        "    model_wording = GradientBoostingRegressor(random_state=42)\n",
        "    model_content = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "    model_wording.fit(X[train_index],y[train_index][:,0])\n",
        "    word_prediction = model_wording.predict(X[test_index])\n",
        "\n",
        "\n",
        "    model_content.fit(X[train_index],y[train_index][:,1])\n",
        "    content_prediction = model_content.predict(X[test_index])\n",
        "\n",
        "    df = pd.DataFrame(index=test_index)\n",
        "    df['true_word'] = y[test_index][:,0]\n",
        "    df['true_content'] = y[test_index][:,1]\n",
        "    df['pred_word'] = word_prediction\n",
        "    df['pred_content'] = content_prediction\n",
        "    dfs1.append(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSmj3PEjPxfS",
        "outputId": "af1a5bcb-c229-4a82-cab7-ea003f919463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begining Fold:1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r1it [00:03,  3.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begining Fold:2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [00:08,  4.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begining Fold:3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [00:12,  4.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begining Fold:4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [00:16,  4.18s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_w = 0.0\n",
        "total_c = 0.0\n",
        "total_mcrmse = 0.0\n",
        "i = 0\n",
        "for df,df1 in (zip(dfs,dfs1)):\n",
        "    df_deberta = pd.read_csv(PATH_TO_FOLDER+f'/predictions_fold_{i+1}.csv',index_col=\"idx\")\n",
        "    word_pred = (df['pred_word'] + df_deberta['pred_word'] + df1['pred_word']) / 3\n",
        "    content_pred = (df['pred_content'] + df_deberta['pred_content'] + df1['pred_content']) / 3\n",
        "    rmse_w = mean_squared_error(df_deberta['true_word'],word_pred,squared=False)\n",
        "    rmse_c = mean_squared_error(df_deberta['true_content'],content_pred,squared=False)\n",
        "    print(f\"Fold {i+1}\")\n",
        "    print(f\"Wording:{rmse_w}\")\n",
        "    print(f\"Content:{rmse_c}\")\n",
        "    print(f\"MCRMSE:{(rmse_w+rmse_c)/2}\")\n",
        "\n",
        "    total_w+=rmse_w\n",
        "    total_c+=rmse_c\n",
        "    total_mcrmse+=(rmse_w+rmse_c)/2\n",
        "    i+=1\n",
        "print(\"Total\")\n",
        "print(f\"Wording:{total_w/4}\")\n",
        "print(f\"Content:{total_c/4}\")\n",
        "print(f\"MCRMSE:{total_mcrmse/4}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLQ_bQnFYFwI",
        "outputId": "2e4e2dc3-c7e1-4ca6-8375-6461fb2c946b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "Wording:0.5275593813358224\n",
            "Content:0.4612529823459557\n",
            "MCRMSE:0.494406181840889\n",
            "Fold 2\n",
            "Wording:0.8200424149352418\n",
            "Content:0.5023597823952067\n",
            "MCRMSE:0.6612010986652243\n",
            "Fold 3\n",
            "Wording:0.5037425587269663\n",
            "Content:0.42972597915880495\n",
            "MCRMSE:0.4667342689428856\n",
            "Fold 4\n",
            "Wording:0.7570528054633436\n",
            "Content:0.611388124796624\n",
            "MCRMSE:0.6842204651299838\n",
            "Total\n",
            "Wording:0.6520992901153435\n",
            "Content:0.5011817171741478\n",
            "MCRMSE:0.5766405036447456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uu_SvIUhQ_NS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
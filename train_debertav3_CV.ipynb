{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0QVlBHXjNug",
        "outputId": "f78ec6d0-bfe9-4aec-c309-470289bade4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA6U-VyZMXdm"
      },
      "source": [
        "### Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q5h6DoELzFt",
        "outputId": "71c7114f-5668-4fed-d5b9-eea70a5ece09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.14.0 transformers-4.34.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.4.1\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.23.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install sentencepiece\n",
        "!pip install textstat\n",
        "!pip install pyspellchecker\n",
        "!pip install datasets\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYTTUlxgMaeE"
      },
      "source": [
        "### Kaggle Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrsPu2YkL2Km",
        "outputId": "34ceb7ad-c260-4f65-ed7e-408cf523c389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading commonlit-evaluate-student-summaries.zip to /content\n",
            " 95% 1.00M/1.05M [00:00<00:00, 1.99MB/s]\n",
            "100% 1.05M/1.05M [00:00<00:00, 2.08MB/s]\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c commonlit-evaluate-student-summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIT6VLpMNo97"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/commonlit-evaluate-student-summaries.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn8aSZlEMq3-"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MHnZN0nLMs1c"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import warnings\n",
        "import logging\n",
        "import os\n",
        "import gc\n",
        "import shutil\n",
        "import json\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,EarlyStoppingCallback\n",
        "from transformers import DataCollatorWithPadding\n",
        "from datasets import Dataset,load_dataset, load_from_disk\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import load_metric, disable_progress_bar\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "from sklearn.model_selection import KFold, GroupKFold\n",
        "from tqdm import tqdm\n",
        "import textstat\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk import pos_tag,ne_chunk, word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import re\n",
        "from spellchecker import SpellChecker\n",
        "import lightgbm as lgb\n",
        "\n",
        "import random\n",
        "# logging setting\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "logging.disable(logging.ERROR)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "disable_progress_bar()\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEW4wqN7MlYx"
      },
      "source": [
        "###Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52oiOSQOfqG8",
        "outputId": "a6bca448-186b-410b-8e2d-6554b148d78d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ctfO27vM7p3"
      },
      "source": [
        "###Set seed to 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjU1fmyLM4vr"
      },
      "outputs": [],
      "source": [
        "# set random seed\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "seed_everything(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKSWcfaQNBVH"
      },
      "source": [
        "###Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_27XghLM-qx"
      },
      "outputs": [],
      "source": [
        "EXP_NUM = 3\n",
        "\n",
        "class CFG:\n",
        "    model_name=\"deberta-v3-base\"\n",
        "    dir_model=\"microsoft/\"\n",
        "    save_model_path = f'/content/{model_name}-model/exp_{EXP_NUM}'\n",
        "    learning_rate=0.000016\n",
        "    weight_decay=0.03\n",
        "    hidden_dropout_prob=0.0\n",
        "    attention_probs_dropout_prob=0.0\n",
        "    num_train_epochs=5\n",
        "    n_splits=4\n",
        "    batch_size=12\n",
        "    random_seed=42\n",
        "    save_steps=100\n",
        "    max_length=1024\n",
        "    early_stopping_patience=25\n",
        "    augmentations=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwx5izVXNLl6"
      },
      "source": [
        "###Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBjFHTMINFen"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/\"\n",
        "\n",
        "prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n",
        "prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n",
        "summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n",
        "summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n",
        "sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8g7AgjlPhGS"
      },
      "source": [
        "###Pre-processor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JHSgfOjPj8Z",
        "outputId": "b8c2459f-0ef2-4115-b481-afc8b7af5662"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53bfaf1ff1944163a0434ba9ae33db59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d9c15b0aaa341a98e41a619707bd1d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87bb68541ce848f9953cb891e39781e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self,model_name: str,dir_model: str) -> None:\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(f\"{dir_model}{model_name}\")\n",
        "        self.STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n",
        "        self.speller = SpellChecker()\n",
        "\n",
        "    def count_text_length(self, df: pd.DataFrame, col:str) -> pd.Series:\n",
        "        \"\"\" text length \"\"\"\n",
        "        tokenizer=self.tokenizer\n",
        "        return df[col].progress_apply(lambda x: len(tokenizer.encode(x)))\n",
        "\n",
        "    #JUST STOP WORDS OVERLAP\n",
        "    def word_overlap_count(self, row):\n",
        "        \"\"\" intersection(prompt_text, text) \"\"\"\n",
        "        def check_is_stop_word(word):\n",
        "            return word in self.STOP_WORDS\n",
        "\n",
        "#         prompt_words = row['prompt_tokens']\n",
        "#         summary_words = row['summary_tokens']\n",
        "\n",
        "        prompt_words = list(self.spacy_ner_model.tokenizer(row['prompt_text']))\n",
        "        summary_words = list(self.spacy_ner_model.tokenizer(row['text']))\n",
        "\n",
        "        prompt_words = [str(word) for word in prompt_words]\n",
        "        summary_words = [str(word) for word in summary_words]\n",
        "\n",
        "        if self.STOP_WORDS:\n",
        "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
        "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
        "        return len(set(prompt_words).intersection(set(summary_words)))\n",
        "\n",
        "    #REAL OVERLAP WORD\n",
        "    def word_overlap_count_real(self, row):\n",
        "        \"\"\" intersection(prompt_text, text) \"\"\"\n",
        "        def check_is_stop_word_real(word):\n",
        "            return word not in self.STOP_WORDS\n",
        "\n",
        "        prompt_words = list(self.spacy_ner_model.tokenizer(row['prompt_text']))\n",
        "        summary_words = list(self.spacy_ner_model.tokenizer(row['text']))\n",
        "\n",
        "        prompt_words = [str(word) for word in prompt_words]\n",
        "        summary_words = [str(word) for word in summary_words]\n",
        "\n",
        "        if self.STOP_WORDS:\n",
        "            prompt_words = list(filter(check_is_stop_word_real, prompt_words))\n",
        "            summary_words = list(filter(check_is_stop_word_real, summary_words))\n",
        "        return len(set(prompt_words).intersection(set(summary_words)))\n",
        "\n",
        "\n",
        "    def ngrams(self, token, n):\n",
        "        # Use the zip function to help us generate n-grams\n",
        "        # Concatentate the tokens into ngrams and return\n",
        "        ngrams = zip(*[token[i:] for i in range(n)])\n",
        "        return [\" \".join(ngram) for ngram in ngrams]\n",
        "\n",
        "    def ngram_co_occurrence(self, row, n: int):\n",
        "        # Tokenize the original text and summary into words\n",
        "        original_tokens = row['prompt_tokens']\n",
        "        summary_tokens = row['summary_tokens']\n",
        "\n",
        "        # Generate n-grams for the original text and summary\n",
        "        original_ngrams = set(self.ngrams(original_tokens, n))\n",
        "        summary_ngrams = set(self.ngrams(summary_tokens, n))\n",
        "\n",
        "        # Calculate the number of common n-grams\n",
        "        common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
        "\n",
        "        # # Optionally, you can get the frequency of common n-grams for a more nuanced analysis\n",
        "        # original_ngram_freq = Counter(ngrams(original_words, n))\n",
        "        # summary_ngram_freq = Counter(ngrams(summary_words, n))\n",
        "        # common_ngram_freq = {ngram: min(original_ngram_freq[ngram], summary_ngram_freq[ngram]) for ngram in common_ngrams}\n",
        "\n",
        "        return len(common_ngrams)\n",
        "\n",
        "    def ner_overlap_count(self, row, mode:str):\n",
        "        model = self.spacy_ner_model\n",
        "        def clean_ners(ner_list):\n",
        "            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n",
        "        prompt = model(row['prompt_text'])\n",
        "        summary = model(row['text'])\n",
        "\n",
        "        if \"spacy\" in str(model):\n",
        "            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n",
        "            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n",
        "        elif \"stanza\" in str(model):\n",
        "            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n",
        "            summary_ner = set([(token.text, token.type) for token in summary.ents])\n",
        "        else:\n",
        "            raise Exception(\"Model not supported\")\n",
        "\n",
        "        prompt_ner = clean_ners(prompt_ner)\n",
        "        summary_ner = clean_ners(summary_ner)\n",
        "\n",
        "        intersecting_ners = prompt_ner.intersection(summary_ner)\n",
        "\n",
        "        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n",
        "\n",
        "        if mode == \"train\":\n",
        "            return ner_dict\n",
        "        elif mode == \"test\":\n",
        "            return {key: ner_dict.get(key) for key in self.ner_keys}\n",
        "\n",
        "\n",
        "    def quotes_count(self, row):\n",
        "        summary = row['text']\n",
        "        text = row['prompt_text']\n",
        "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
        "        if len(quotes_from_summary)>0:\n",
        "            return [quote in text for quote in quotes_from_summary].count(True)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def spelling(self, text):\n",
        "\n",
        "#         wordlist=text.split()\n",
        "        wordlist = self.spacy_ner_model.tokenizer(text)\n",
        "        wordlist = [str(word) for word in wordlist]\n",
        "        amount_miss = len(list(self.speller.unknown(wordlist)))\n",
        "\n",
        "        return amount_miss\n",
        "\n",
        "\n",
        "    def calculate_pos_ratios(self,text):\n",
        "        pos_tags = pos_tag(nltk.word_tokenize(text))\n",
        "        pos_counts = Counter(tag for word, tag in pos_tags)\n",
        "        total_words = len(pos_tags)\n",
        "        ratios = {tag: count / total_words for tag, count in pos_counts.items()}\n",
        "        return ratios\n",
        "\n",
        "    def calculate_sentiment_scores(self,text):\n",
        "        sid = SentimentIntensityAnalyzer()\n",
        "        sentiment_scores = sid.polarity_scores(text)\n",
        "        return sentiment_scores\n",
        "\n",
        "    def calculate_punctuation_ratios(self,text):\n",
        "        total_chars = len(text)\n",
        "        punctuation_counts = Counter(char for char in text if char in '.,!?;:\"()[]{}')\n",
        "        ratios = {char: count / total_chars for char, count in punctuation_counts.items()}\n",
        "        return ratios\n",
        "\n",
        "    def calculate_keyword_density(self,row):\n",
        "        keywords = set(row['prompt_text'].split())\n",
        "        text_words = row['text'].split()\n",
        "        keyword_count = sum(1 for word in text_words if word in keywords)\n",
        "        return keyword_count / len(text_words)\n",
        "\n",
        "\n",
        "    def run(self,prompts: pd.DataFrame,summaries:pd.DataFrame,mode:str) -> pd.DataFrame:\n",
        "\n",
        "        # before merge preprocess\n",
        "\n",
        "#         prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n",
        "#             lambda x: len(self.tokenizer.encode(x))\n",
        "#         )\n",
        "\n",
        "        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n",
        "            lambda x: len(list(self.spacy_ner_model.tokenizer(x)))\n",
        "        )\n",
        "\n",
        "        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n",
        "            lambda x: self.tokenizer.convert_ids_to_tokens(\n",
        "                self.tokenizer.encode(x),\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "        )\n",
        "\n",
        "#         summaries[\"summary_length\"] = summaries[\"text\"].apply(\n",
        "#             lambda x: len(self.tokenizer.encode(x))\n",
        "#         )\n",
        "\n",
        "        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n",
        "            lambda x: len(list(self.spacy_ner_model.tokenizer(x)))\n",
        "        )\n",
        "\n",
        "        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n",
        "            lambda x: self.tokenizer.convert_ids_to_tokens(\n",
        "                self.tokenizer.encode(x),\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "        )\n",
        "        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n",
        "\n",
        "        # merge prompts and summaries\n",
        "        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n",
        "\n",
        "        # after merge preprocess\n",
        "        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n",
        "\n",
        "        #stop words overlap\n",
        "        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n",
        "\n",
        "\n",
        "        input_df['bigram_overlap_count'] = input_df.progress_apply(\n",
        "            self.ngram_co_occurrence,args=(2,), axis=1\n",
        "        )\n",
        "        input_df['trigram_overlap_count'] = input_df.progress_apply(\n",
        "            self.ngram_co_occurrence, args=(3,), axis=1\n",
        "        )\n",
        "\n",
        "#         # Crate dataframe with count of each category NERs overlap for all the summaries\n",
        "#         # Because it spends too much time for this feature, I don't use this time.\n",
        "#         ners_count_df  = input_df.progress_apply(\n",
        "#             lambda row: pd.Series(self.ner_overlap_count(row, mode=mode), dtype='float64'), axis=1\n",
        "#         ).fillna(0)\n",
        "#         self.ner_keys = ners_count_df.columns\n",
        "#         ners_count_df['sum'] = ners_count_df.sum(axis=1)\n",
        "#         ners_count_df.columns = ['NER_' + col for col in ners_count_df.columns]\n",
        "#         # join ner count dataframe with train dataframe\n",
        "#         input_df = pd.concat([input_df, ners_count_df], axis=1)\n",
        "\n",
        "        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n",
        "\n",
        "        #Additional\n",
        "\n",
        "        #real overlap words\n",
        "        input_df['real_word_overlap_count'] = input_df.progress_apply(self.word_overlap_count_real, axis=1)\n",
        "\n",
        "        input_df['sentence_length'] = input_df['text'].progress_apply(lambda x: len(x.split('.')))\n",
        "        input_df['vocabulary_richness'] = input_df['text'].progress_apply(lambda x: len(set(x.split())))\n",
        "        input_df['avg_word_length'] = input_df['text'].progress_apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
        "        input_df['comma_count'] = input_df['text'].progress_apply(lambda x: x.count(','))\n",
        "        input_df['semicolon_count'] = input_df['text'].progress_apply(lambda x: x.count(';'))\n",
        "\n",
        "        input_df['pos_ratios'] = input_df['text'].progress_apply(self.calculate_pos_ratios)\n",
        "        input_df['pos_mean'] = input_df['pos_ratios'].progress_apply(lambda x: np.mean(list(x.values())))\n",
        "\n",
        "        input_df['sentiment_scores'] = input_df['text'].progress_apply(self.calculate_sentiment_scores)\n",
        "\n",
        "        sentiment_columns = pd.DataFrame(list(input_df['sentiment_scores']))\n",
        "        input_df = pd.concat([input_df, sentiment_columns], axis=1)\n",
        "\n",
        "        input_df['exclamation_count'] = input_df['text'].progress_apply(lambda x: x.count('!'))\n",
        "        input_df['question_count'] = input_df['text'].progress_apply(lambda x: x.count('?'))\n",
        "        input_df['quote_count'] = input_df['text'].progress_apply(lambda x: x.count('\"'))\n",
        "\n",
        "        input_df['punctuation_ratios'] = input_df['text'].progress_apply(self.calculate_punctuation_ratios)\n",
        "        input_df['punctuation_sum'] = input_df['punctuation_ratios'].progress_apply(lambda x: np.sum(list(x.values())))\n",
        "\n",
        "        input_df['keyword_density'] = input_df.progress_apply(self.calculate_keyword_density, axis=1)\n",
        "\n",
        "        input_df['sentiment_scores_prompt'] = input_df['prompt_text'].progress_apply(self.calculate_sentiment_scores)\n",
        "\n",
        "        sentiment_columns_prompt = pd.DataFrame(list(input_df['sentiment_scores_prompt']))\n",
        "        sentiment_columns_prompt.columns = [col +'_prompt' for col in sentiment_columns_prompt.columns]\n",
        "\n",
        "        input_df = pd.concat([input_df, sentiment_columns_prompt], axis=1)\n",
        "\n",
        "        input_df['jaccard_similarity'] = input_df.progress_apply(lambda row: len(set(word_tokenize(row['prompt_text'])) & set(word_tokenize(row['text']))) / len(set(word_tokenize(row['prompt_text'])) | set(word_tokenize(row['text']))), axis=1)\n",
        "\n",
        "\n",
        "        ###########TEXTSTAT FEARURES#############\n",
        "        input_df['flesch_reading_ease'] = input_df['text'].progress_apply(lambda x: textstat.flesch_reading_ease(x))\n",
        "        input_df['flesch_kincaid_grade'] = input_df['text'].progress_apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
        "        input_df['gunning_fog'] = input_df['text'].progress_apply(lambda x: textstat.gunning_fog(x))\n",
        "        #input_df['smog_index'] = input_df['text'].progress_apply(lambda x: textstat.smog_index(x))\n",
        "        input_df['automated_readability_index'] = input_df['text'].progress_apply(lambda x: textstat.automated_readability_index(x))\n",
        "        input_df['coleman_liau_index'] = input_df['text'].progress_apply(lambda x: textstat.coleman_liau_index(x))\n",
        "        input_df['linsear_write_formula'] = input_df['text'].progress_apply(lambda x: textstat.linsear_write_formula(x))\n",
        "        input_df['dale_chall_readability_score'] = input_df['text'].progress_apply(lambda x: textstat.dale_chall_readability_score(x))\n",
        "        input_df['text_standard'] = input_df['text'].progress_apply(lambda x: textstat.text_standard(x,float_output=True))\n",
        "        input_df['spache_readability'] = input_df['text'].progress_apply(lambda x: textstat.spache_readability(x))\n",
        "        input_df['mcalpine_eflaw'] = input_df['text'].progress_apply(lambda x: textstat.mcalpine_eflaw(x))\n",
        "        input_df['reading_time'] = input_df['text'].progress_apply(lambda x: textstat.reading_time(x))\n",
        "        input_df['syllable_count'] = input_df['text'].progress_apply(lambda x: textstat.syllable_count(x))\n",
        "        input_df['polysyllabcount'] = input_df['text'].progress_apply(lambda x: textstat.polysyllabcount(x))\n",
        "        input_df['monosyllabcount'] = input_df['text'].progress_apply(lambda x: textstat.monosyllabcount(x))\n",
        "\n",
        "\n",
        "        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\",\"pos_ratios\",\"sentiment_scores\",\"punctuation_ratios\",\"sentiment_scores_prompt\"])\n",
        "\n",
        "preprocessor = Preprocessor(model_name=CFG.model_name,dir_model=CFG.dir_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8id7693RhaR"
      },
      "source": [
        "###Train dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fml390ZWQ49D",
        "outputId": "a6bf9b47-5402-4c6d-8b46-8a18a227c336"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1e57e5fe-6e98-4636-9edc-a51b8ff12977\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>student_id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>text</th>\n",
              "      <th>content</th>\n",
              "      <th>wording</th>\n",
              "      <th>summary_length</th>\n",
              "      <th>splling_err_num</th>\n",
              "      <th>prompt_question</th>\n",
              "      <th>prompt_title</th>\n",
              "      <th>prompt_text</th>\n",
              "      <th>...</th>\n",
              "      <th>coleman_liau_index</th>\n",
              "      <th>linsear_write_formula</th>\n",
              "      <th>dale_chall_readability_score</th>\n",
              "      <th>text_standard</th>\n",
              "      <th>spache_readability</th>\n",
              "      <th>mcalpine_eflaw</th>\n",
              "      <th>reading_time</th>\n",
              "      <th>syllable_count</th>\n",
              "      <th>polysyllabcount</th>\n",
              "      <th>monosyllabcount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000e8c3c7ddb</td>\n",
              "      <td>814d6b</td>\n",
              "      <td>The third wave was an experimentto see how peo...</td>\n",
              "      <td>0.205683</td>\n",
              "      <td>0.380538</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>Summarize how the Third Wave developed over su...</td>\n",
              "      <td>The Third Wave</td>\n",
              "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
              "      <td>...</td>\n",
              "      <td>9.04</td>\n",
              "      <td>8.375</td>\n",
              "      <td>7.76</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.54</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.20</td>\n",
              "      <td>93</td>\n",
              "      <td>7</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0020ae56ffbf</td>\n",
              "      <td>ebad26</td>\n",
              "      <td>They would rub it up with soda to make the sme...</td>\n",
              "      <td>-0.548304</td>\n",
              "      <td>0.506755</td>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>Summarize the various ways the factory would u...</td>\n",
              "      <td>Excerpt from The Jungle</td>\n",
              "      <td>With one member trimming beef in a cannery, an...</td>\n",
              "      <td>...</td>\n",
              "      <td>4.30</td>\n",
              "      <td>13.000</td>\n",
              "      <td>6.44</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>38.5</td>\n",
              "      <td>2.84</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>004e978e639e</td>\n",
              "      <td>3b9047</td>\n",
              "      <td>In Egypt, there were many occupations and soci...</td>\n",
              "      <td>3.128928</td>\n",
              "      <td>4.231226</td>\n",
              "      <td>275</td>\n",
              "      <td>3</td>\n",
              "      <td>In complete sentences, summarize the structure...</td>\n",
              "      <td>Egyptian Social Structure</td>\n",
              "      <td>Egyptian society was structured like a pyramid...</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92</td>\n",
              "      <td>11.200</td>\n",
              "      <td>8.32</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.95</td>\n",
              "      <td>26.8</td>\n",
              "      <td>16.69</td>\n",
              "      <td>317</td>\n",
              "      <td>14</td>\n",
              "      <td>170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>005ab0199905</td>\n",
              "      <td>3b9047</td>\n",
              "      <td>The highest class was Pharaohs these people we...</td>\n",
              "      <td>-0.210614</td>\n",
              "      <td>-0.471415</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>In complete sentences, summarize the structure...</td>\n",
              "      <td>Egyptian Social Structure</td>\n",
              "      <td>Egyptian society was structured like a pyramid...</td>\n",
              "      <td>...</td>\n",
              "      <td>10.11</td>\n",
              "      <td>4.500</td>\n",
              "      <td>11.63</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.39</td>\n",
              "      <td>11.3</td>\n",
              "      <td>1.95</td>\n",
              "      <td>37</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0070c9e7af47</td>\n",
              "      <td>814d6b</td>\n",
              "      <td>The Third Wave developed  rapidly because the ...</td>\n",
              "      <td>3.272894</td>\n",
              "      <td>3.219757</td>\n",
              "      <td>236</td>\n",
              "      <td>15</td>\n",
              "      <td>Summarize how the Third Wave developed over su...</td>\n",
              "      <td>The Third Wave</td>\n",
              "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
              "      <td>...</td>\n",
              "      <td>10.43</td>\n",
              "      <td>6.625</td>\n",
              "      <td>8.24</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.32</td>\n",
              "      <td>20.2</td>\n",
              "      <td>14.98</td>\n",
              "      <td>301</td>\n",
              "      <td>21</td>\n",
              "      <td>136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 51 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e57e5fe-6e98-4636-9edc-a51b8ff12977')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1e57e5fe-6e98-4636-9edc-a51b8ff12977 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1e57e5fe-6e98-4636-9edc-a51b8ff12977');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6d694ce3-bd3f-4b13-ba91-5e0b273c31af\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6d694ce3-bd3f-4b13-ba91-5e0b273c31af')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6d694ce3-bd3f-4b13-ba91-5e0b273c31af button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     student_id prompt_id                                               text  \\\n",
              "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
              "1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
              "2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n",
              "3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n",
              "4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
              "\n",
              "    content   wording  summary_length  splling_err_num  \\\n",
              "0  0.205683  0.380538              64                2   \n",
              "1 -0.548304  0.506755              55                1   \n",
              "2  3.128928  4.231226             275                3   \n",
              "3 -0.210614 -0.471415              32                3   \n",
              "4  3.272894  3.219757             236               15   \n",
              "\n",
              "                                     prompt_question  \\\n",
              "0  Summarize how the Third Wave developed over su...   \n",
              "1  Summarize the various ways the factory would u...   \n",
              "2  In complete sentences, summarize the structure...   \n",
              "3  In complete sentences, summarize the structure...   \n",
              "4  Summarize how the Third Wave developed over su...   \n",
              "\n",
              "                prompt_title  \\\n",
              "0             The Third Wave   \n",
              "1    Excerpt from The Jungle   \n",
              "2  Egyptian Social Structure   \n",
              "3  Egyptian Social Structure   \n",
              "4             The Third Wave   \n",
              "\n",
              "                                         prompt_text  ...  coleman_liau_index  \\\n",
              "0  Background \\r\\nThe Third Wave experiment took ...  ...                9.04   \n",
              "1  With one member trimming beef in a cannery, an...  ...                4.30   \n",
              "2  Egyptian society was structured like a pyramid...  ...                9.92   \n",
              "3  Egyptian society was structured like a pyramid...  ...               10.11   \n",
              "4  Background \\r\\nThe Third Wave experiment took ...  ...               10.43   \n",
              "\n",
              "   linsear_write_formula  dale_chall_readability_score  text_standard  \\\n",
              "0                  8.375                          7.76            8.0   \n",
              "1                 13.000                          6.44            8.0   \n",
              "2                 11.200                          8.32            9.0   \n",
              "3                  4.500                         11.63            5.0   \n",
              "4                  6.625                          8.24           10.0   \n",
              "\n",
              "   spache_readability  mcalpine_eflaw  reading_time  syllable_count  \\\n",
              "0                4.54            22.0          4.20              93   \n",
              "1                5.00            38.5          2.84              56   \n",
              "2                4.95            26.8         16.69             317   \n",
              "3                3.39            11.3          1.95              37   \n",
              "4                4.32            20.2         14.98             301   \n",
              "\n",
              "   polysyllabcount  monosyllabcount  \n",
              "0                7               40  \n",
              "1                0               48  \n",
              "2               14              170  \n",
              "3                4               18  \n",
              "4               21              136  \n",
              "\n",
              "[5 rows x 51 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n",
        "# test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/CommonLit/commont-lit-train.csv\")\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWw-Uzf-l1xA"
      },
      "source": [
        "###Add Augmentations if True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-rsDuKNl8zC"
      },
      "outputs": [],
      "source": [
        "if CFG.augmentations:\n",
        "    train_aug = pd.read_csv(\"/content/drive/MyDrive/CommonLit/train_back_translation.csv\")\n",
        "    train_aug = train_aug.drop(train_aug.columns[11:],axis=1)\n",
        "    train_aug = train_aug.drop(['summary_length','splling_err_num','prompt_length'],axis=1)\n",
        "    train_aug['text'] = train_aug['text'].apply(lambda x: x[2:-2])\n",
        "    train_aug['augmentation'] = 'yes'\n",
        "    train['augmentation'] = 'no'\n",
        "    train = pd.concat([train,train_aug],axis=0)\n",
        "    train = train.reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCMOX6R8Rnwf"
      },
      "source": [
        "###Group K-Fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI0fzPggRsJM"
      },
      "outputs": [],
      "source": [
        "gkf = GroupKFold(n_splits=CFG.n_splits)\n",
        "\n",
        "for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n",
        "    train.loc[val_index, \"fold\"] = i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3z0L4BvRxN-"
      },
      "source": [
        "###Metrics computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJGg_5GHRy_L"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
        "    return {\"rmse\": rmse}\n",
        "\n",
        "def compute_mcrmse(eval_pred):\n",
        "    \"\"\"\n",
        "    Calculates mean columnwise root mean squared error\n",
        "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
        "    \"\"\"\n",
        "    preds, labels = eval_pred\n",
        "\n",
        "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
        "    mcrmse = np.mean(col_rmse)\n",
        "\n",
        "    return {\n",
        "        \"content_rmse\": col_rmse[0],\n",
        "        \"wording_rmse\": col_rmse[1],\n",
        "        \"mcrmse\": mcrmse,\n",
        "    }\n",
        "\n",
        "def compt_score(content_true, content_pred, wording_true, wording_pred):\n",
        "    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n",
        "    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n",
        "\n",
        "    return (content_score + wording_score)/2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZNPjFyvR-r4"
      },
      "source": [
        "###Loss defenition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBlx1oWQR-FQ"
      },
      "outputs": [],
      "source": [
        "def mcrmse_loss(y_true, y_pred):\n",
        "    colwise_mse = torch.mean(torch.square(y_true - y_pred), dim=0)\n",
        "    return torch.mean(torch.sqrt(colwise_mse), dim=0)\n",
        "\n",
        "# def mse_loss()\n",
        "#     pass\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = mcrmse_loss(labels, outputs['logits'])\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPma9z5BTNDe"
      },
      "source": [
        "###Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-acPnG3pTPqX"
      },
      "outputs": [],
      "source": [
        "class ScoreRegressor:\n",
        "    def __init__(self,model_name: str,dir_model:str, model_dir: str,inputs: List[str],target_cols: List[str],hidden_dropout_prob: float,\n",
        "                attention_probs_dropout_prob: float, max_length: int,):\n",
        "\n",
        "        self.input_col = \"input\" # col name of model input after text concat sep token\n",
        "        self.input_text_cols = inputs\n",
        "        self.target_cols = target_cols\n",
        "        self.model_name = model_name\n",
        "        self.dir_model = dir_model\n",
        "        self.model_dir = model_dir\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(f\"{dir_model}{model_name}\")\n",
        "        self.model_config = AutoConfig.from_pretrained(f\"{dir_model}{model_name}\")\n",
        "\n",
        "        self.model_config.update({\n",
        "            \"hidden_dropout_prob\": hidden_dropout_prob,\n",
        "            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n",
        "            \"num_labels\": 2,\n",
        "            \"problem_type\": \"regression\",\n",
        "        })\n",
        "\n",
        "        self.data_collator = DataCollatorWithPadding(\n",
        "            tokenizer=self.tokenizer\n",
        "        )\n",
        "\n",
        "    def concatenate_with_sep_token(self, row):\n",
        "        sep = \" \" + self.tokenizer.sep_token + \" \"\n",
        "        return sep.join(row[self.input_text_cols])\n",
        "\n",
        "    def tokenize_function(self, examples: pd.DataFrame):\n",
        "        labels = [examples[\"content\"], examples[\"wording\"]]\n",
        "        tokenized = self.tokenizer(examples[self.input_col],\n",
        "                        padding=\"max_length\",\n",
        "                        truncation=True,\n",
        "                        max_length=self.max_length)\n",
        "        return {\n",
        "            **tokenized,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "\n",
        "    def tokenize_function_test(self, examples: pd.DataFrame):\n",
        "        tokenized = self.tokenizer(examples[self.input_col],\n",
        "                        padding=\"max_length\",\n",
        "                        truncation=True,\n",
        "                        max_length=self.max_length)\n",
        "        return tokenized\n",
        "\n",
        "    def train(self,fold: int,train_df: pd.DataFrame,valid_df: pd.DataFrame,batch_size: int,learning_rate: float,\n",
        "            weight_decay: float,num_train_epochs: float,save_steps: int,) -> None:\n",
        "        \"\"\"fine-tuning\"\"\"\n",
        "\n",
        "        train_df[self.input_col] = train_df.apply(self.concatenate_with_sep_token, axis=1)\n",
        "        valid_df[self.input_col] = valid_df.apply(self.concatenate_with_sep_token, axis=1)\n",
        "\n",
        "        train_df = train_df[[self.input_col] + self.target_cols]\n",
        "        valid_df = valid_df[[self.input_col] + self.target_cols]\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            f\"{self.dir_model}{self.model_name}\",\n",
        "            config=self.model_config\n",
        "        )\n",
        "\n",
        "        train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False)\n",
        "\n",
        "        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n",
        "        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n",
        "\n",
        "        # eg. \"bert/fold_0/\"\n",
        "        model_fold_dir = os.path.join(self.model_dir, str(fold))\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=model_fold_dir,\n",
        "            overwrite_output_dir=True,\n",
        "            do_train=True,\n",
        "            load_best_model_at_end=True, # select best model\n",
        "            learning_rate=learning_rate,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            weight_decay=weight_decay,\n",
        "            report_to='none',\n",
        "            greater_is_better=False,\n",
        "            save_strategy=\"steps\",\n",
        "            evaluation_strategy=\"steps\",\n",
        "            eval_steps=save_steps,\n",
        "            save_steps=save_steps,\n",
        "            metric_for_best_model=\"mcrmse\",\n",
        "            save_total_limit=1,\n",
        "            fp16=True,\n",
        "            auto_find_batch_size=True,\n",
        "        )\n",
        "\n",
        "        trainer = CustomTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_tokenized_datasets,\n",
        "            eval_dataset=val_tokenized_datasets,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=compute_mcrmse,\n",
        "            data_collator=self.data_collator,\n",
        "            callbacks = [EarlyStoppingCallback(early_stopping_patience=CFG.early_stopping_patience)]\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        # model.save_pretrained(self.model_dir)\n",
        "        # self.tokenizer.save_pretrained(self.model_dir)\n",
        "\n",
        "        model.cpu()\n",
        "        del model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    def predict(self,test_df: pd.DataFrame,batch_size: int,fold: int,):\n",
        "        \"\"\"predict content score\"\"\"\n",
        "\n",
        "        test_df[self.input_col] = test_df.apply(self.concatenate_with_sep_token, axis=1)\n",
        "\n",
        "        test_dataset = Dataset.from_pandas(test_df[[self.input_col]], preserve_index=False)\n",
        "        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n",
        "\n",
        "\n",
        "\n",
        "        checkpoint_folder = os.listdir(CFG.save_model_path+f\"/fold_{str(fold)}/{str(fold)}\")\n",
        "\n",
        "        model_fold_dir = os.path.join(CFG.save_model_path+f\"/fold_{str(fold)}/{str(fold)}\",checkpoint_folder[0])\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_fold_dir)\n",
        "        model.eval()\n",
        "\n",
        "        # e.g. \"bert/fold_0/\"\n",
        "\n",
        "        test_args = TrainingArguments(\n",
        "            output_dir=model_fold_dir,\n",
        "            do_train=False,\n",
        "            do_predict=True,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            dataloader_drop_last=False,\n",
        "            fp16=True,\n",
        "            auto_find_batch_size=True,\n",
        "        )\n",
        "\n",
        "        # init trainer\n",
        "        infer_content = CustomTrainer(\n",
        "                      model = model,\n",
        "                      tokenizer=self.tokenizer,\n",
        "                      data_collator=self.data_collator,\n",
        "                      args = test_args)\n",
        "\n",
        "        preds = infer_content.predict(test_tokenized_dataset)[0]\n",
        "        pred_df = pd.DataFrame(\n",
        "            preds,\n",
        "            columns=[\n",
        "                f\"content_pred\",\n",
        "                f\"wording_pred\"\n",
        "           ]\n",
        "        )\n",
        "\n",
        "        model.cpu()\n",
        "        del model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return pred_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlF-r33-cgzM"
      },
      "outputs": [],
      "source": [
        "def train_by_fold(\n",
        "        train_df: pd.DataFrame,\n",
        "        model_name: str,\n",
        "        dir_model:str,\n",
        "        targets: List[str],\n",
        "        inputs: List[str],\n",
        "        save_each_model: bool,\n",
        "        n_splits: int,\n",
        "        batch_size: int,\n",
        "        learning_rate: int,\n",
        "        hidden_dropout_prob: float,\n",
        "        attention_probs_dropout_prob: float,\n",
        "        weight_decay: float,\n",
        "        num_train_epochs: int,\n",
        "        save_steps: int,\n",
        "        max_length:int\n",
        "    ):\n",
        "\n",
        "    # delete old model files\n",
        "    # if os.path.exists(model_name):\n",
        "    #     shutil.rmtree(model_name)\n",
        "\n",
        "    os.makedirs(CFG.save_model_path)\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        print(f\"fold {fold}:\")\n",
        "\n",
        "        train_data = train_df[train_df[\"fold\"] != fold]\n",
        "        if CFG.augmentations:\n",
        "            valid_data = train_df[train_df[\"fold\"] == fold]\n",
        "            valid_data = valid_data[valid_data['augmentation']=='no']\n",
        "        else:\n",
        "            valid_data = train_df[train_df[\"fold\"] == fold]\n",
        "\n",
        "        model_dir =  CFG.save_model_path + f\"/fold_{fold}\"\n",
        "\n",
        "        csr = ScoreRegressor(\n",
        "            model_name=model_name,\n",
        "            dir_model=dir_model,\n",
        "            target_cols=targets,\n",
        "            inputs= inputs,\n",
        "            model_dir = model_dir,\n",
        "            hidden_dropout_prob=hidden_dropout_prob,\n",
        "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
        "            max_length=max_length,\n",
        "           )\n",
        "\n",
        "        csr.train(\n",
        "            fold=fold,\n",
        "            train_df=train_data,\n",
        "            valid_df=valid_data,\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            save_steps=save_steps,\n",
        "        )\n",
        "\n",
        "def validate(\n",
        "    train_df: pd.DataFrame,\n",
        "    mode: str,\n",
        "    targets: List[str],\n",
        "    inputs: List[str],\n",
        "    save_each_model: bool,\n",
        "    n_splits: int,\n",
        "    batch_size: int,\n",
        "    model_name: str,\n",
        "    dir_model:str,\n",
        "    hidden_dropout_prob: float,\n",
        "    attention_probs_dropout_prob: float,\n",
        "    max_length : int\n",
        "    ) -> pd.DataFrame:\n",
        "    \"\"\"predict oof data\"\"\"\n",
        "\n",
        "    columns = list(train_df.columns.values)\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        print(f\"fold {fold}:\")\n",
        "\n",
        "        valid_data = train_df[train_df[\"fold\"] == fold]\n",
        "\n",
        "        model_dir =  f\"{model_name}/fold_{fold}\"\n",
        "\n",
        "        csr = ScoreRegressor(\n",
        "            model_name=model_name,\n",
        "            dir_model=dir_model,\n",
        "            target_cols=targets,\n",
        "            inputs= inputs,\n",
        "            model_dir = model_dir,\n",
        "            hidden_dropout_prob=hidden_dropout_prob,\n",
        "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
        "            max_length=max_length,\n",
        "           )\n",
        "\n",
        "        pred_df = csr.predict(\n",
        "            test_df=valid_data,\n",
        "            batch_size=batch_size,\n",
        "            fold=fold\n",
        "        )\n",
        "\n",
        "        train_df.loc[valid_data.index, f\"content_{mode}_pred\"] = pred_df[f\"content_pred\"].values\n",
        "        train_df.loc[valid_data.index, f\"wording_{mode}_pred\"] = pred_df[f\"wording_pred\"].values\n",
        "\n",
        "    return train_df[columns + [f\"content_{mode}_pred\", f\"wording_{mode}_pred\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aPdYn-ImX0L"
      },
      "source": [
        "###Train and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 783,
          "referenced_widgets": [
            "42b321c7f334436aabfdb8a91ff8e3f9"
          ]
        },
        "id": "AZrQ4QDreRn2",
        "outputId": "c67269a5-307a-4520-b6d8-2fc7322a6f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold 0:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42b321c7f334436aabfdb8a91ff8e3f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deberta.embeddings.word_embeddings.weight False\n",
            "deberta.embeddings.LayerNorm.weight False\n",
            "deberta.embeddings.LayerNorm.bias False\n",
            "deberta.encoder.layer.0.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.0.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.0.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.0.attention.output.dense.weight False\n",
            "deberta.encoder.layer.0.attention.output.dense.bias False\n",
            "deberta.encoder.layer.0.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.0.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.0.intermediate.dense.weight False\n",
            "deberta.encoder.layer.0.intermediate.dense.bias False\n",
            "deberta.encoder.layer.0.output.dense.weight False\n",
            "deberta.encoder.layer.0.output.dense.bias False\n",
            "deberta.encoder.layer.0.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.0.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.1.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.1.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.1.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.1.attention.output.dense.weight False\n",
            "deberta.encoder.layer.1.attention.output.dense.bias False\n",
            "deberta.encoder.layer.1.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.1.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.1.intermediate.dense.weight False\n",
            "deberta.encoder.layer.1.intermediate.dense.bias False\n",
            "deberta.encoder.layer.1.output.dense.weight False\n",
            "deberta.encoder.layer.1.output.dense.bias False\n",
            "deberta.encoder.layer.1.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.1.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.2.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.2.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.2.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.2.attention.output.dense.weight False\n",
            "deberta.encoder.layer.2.attention.output.dense.bias False\n",
            "deberta.encoder.layer.2.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.2.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.2.intermediate.dense.weight False\n",
            "deberta.encoder.layer.2.intermediate.dense.bias False\n",
            "deberta.encoder.layer.2.output.dense.weight False\n",
            "deberta.encoder.layer.2.output.dense.bias False\n",
            "deberta.encoder.layer.2.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.2.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.3.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.3.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.3.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.3.attention.output.dense.weight False\n",
            "deberta.encoder.layer.3.attention.output.dense.bias False\n",
            "deberta.encoder.layer.3.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.3.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.3.intermediate.dense.weight False\n",
            "deberta.encoder.layer.3.intermediate.dense.bias False\n",
            "deberta.encoder.layer.3.output.dense.weight False\n",
            "deberta.encoder.layer.3.output.dense.bias False\n",
            "deberta.encoder.layer.3.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.3.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.4.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.4.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.4.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.4.attention.output.dense.weight False\n",
            "deberta.encoder.layer.4.attention.output.dense.bias False\n",
            "deberta.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.4.intermediate.dense.weight False\n",
            "deberta.encoder.layer.4.intermediate.dense.bias False\n",
            "deberta.encoder.layer.4.output.dense.weight False\n",
            "deberta.encoder.layer.4.output.dense.bias False\n",
            "deberta.encoder.layer.4.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.4.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.5.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.5.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.5.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.5.attention.output.dense.weight False\n",
            "deberta.encoder.layer.5.attention.output.dense.bias False\n",
            "deberta.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.5.intermediate.dense.weight False\n",
            "deberta.encoder.layer.5.intermediate.dense.bias False\n",
            "deberta.encoder.layer.5.output.dense.weight False\n",
            "deberta.encoder.layer.5.output.dense.bias False\n",
            "deberta.encoder.layer.5.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.5.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.6.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.6.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.6.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.6.attention.output.dense.weight True\n",
            "deberta.encoder.layer.6.attention.output.dense.bias True\n",
            "deberta.encoder.layer.6.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.6.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.6.intermediate.dense.weight True\n",
            "deberta.encoder.layer.6.intermediate.dense.bias True\n",
            "deberta.encoder.layer.6.output.dense.weight True\n",
            "deberta.encoder.layer.6.output.dense.bias True\n",
            "deberta.encoder.layer.6.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.6.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.7.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.7.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.7.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.7.attention.output.dense.weight True\n",
            "deberta.encoder.layer.7.attention.output.dense.bias True\n",
            "deberta.encoder.layer.7.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.7.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.7.intermediate.dense.weight True\n",
            "deberta.encoder.layer.7.intermediate.dense.bias True\n",
            "deberta.encoder.layer.7.output.dense.weight True\n",
            "deberta.encoder.layer.7.output.dense.bias True\n",
            "deberta.encoder.layer.7.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.7.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.8.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.8.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.8.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.8.attention.output.dense.weight True\n",
            "deberta.encoder.layer.8.attention.output.dense.bias True\n",
            "deberta.encoder.layer.8.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.8.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.8.intermediate.dense.weight True\n",
            "deberta.encoder.layer.8.intermediate.dense.bias True\n",
            "deberta.encoder.layer.8.output.dense.weight True\n",
            "deberta.encoder.layer.8.output.dense.bias True\n",
            "deberta.encoder.layer.8.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.8.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.9.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.9.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.9.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.9.attention.output.dense.weight True\n",
            "deberta.encoder.layer.9.attention.output.dense.bias True\n",
            "deberta.encoder.layer.9.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.9.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.9.intermediate.dense.weight True\n",
            "deberta.encoder.layer.9.intermediate.dense.bias True\n",
            "deberta.encoder.layer.9.output.dense.weight True\n",
            "deberta.encoder.layer.9.output.dense.bias True\n",
            "deberta.encoder.layer.9.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.9.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.10.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.10.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.10.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.10.attention.output.dense.weight True\n",
            "deberta.encoder.layer.10.attention.output.dense.bias True\n",
            "deberta.encoder.layer.10.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.10.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.10.intermediate.dense.weight True\n",
            "deberta.encoder.layer.10.intermediate.dense.bias True\n",
            "deberta.encoder.layer.10.output.dense.weight True\n",
            "deberta.encoder.layer.10.output.dense.bias True\n",
            "deberta.encoder.layer.10.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.10.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.11.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.11.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.11.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.11.attention.output.dense.weight True\n",
            "deberta.encoder.layer.11.attention.output.dense.bias True\n",
            "deberta.encoder.layer.11.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.11.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.11.intermediate.dense.weight True\n",
            "deberta.encoder.layer.11.intermediate.dense.bias True\n",
            "deberta.encoder.layer.11.output.dense.weight True\n",
            "deberta.encoder.layer.11.output.dense.bias True\n",
            "deberta.encoder.layer.11.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.11.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.12.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.12.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.12.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.12.attention.output.dense.weight True\n",
            "deberta.encoder.layer.12.attention.output.dense.bias True\n",
            "deberta.encoder.layer.12.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.12.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.12.intermediate.dense.weight True\n",
            "deberta.encoder.layer.12.intermediate.dense.bias True\n",
            "deberta.encoder.layer.12.output.dense.weight True\n",
            "deberta.encoder.layer.12.output.dense.bias True\n",
            "deberta.encoder.layer.12.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.12.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.13.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.13.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.13.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.13.attention.output.dense.weight True\n",
            "deberta.encoder.layer.13.attention.output.dense.bias True\n",
            "deberta.encoder.layer.13.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.13.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.13.intermediate.dense.weight True\n",
            "deberta.encoder.layer.13.intermediate.dense.bias True\n",
            "deberta.encoder.layer.13.output.dense.weight True\n",
            "deberta.encoder.layer.13.output.dense.bias True\n",
            "deberta.encoder.layer.13.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.13.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.14.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.14.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.14.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.14.attention.output.dense.weight True\n",
            "deberta.encoder.layer.14.attention.output.dense.bias True\n",
            "deberta.encoder.layer.14.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.14.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.14.intermediate.dense.weight True\n",
            "deberta.encoder.layer.14.intermediate.dense.bias True\n",
            "deberta.encoder.layer.14.output.dense.weight True\n",
            "deberta.encoder.layer.14.output.dense.bias True\n",
            "deberta.encoder.layer.14.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.14.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.15.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.15.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.15.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.15.attention.output.dense.weight True\n",
            "deberta.encoder.layer.15.attention.output.dense.bias True\n",
            "deberta.encoder.layer.15.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.15.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.15.intermediate.dense.weight True\n",
            "deberta.encoder.layer.15.intermediate.dense.bias True\n",
            "deberta.encoder.layer.15.output.dense.weight True\n",
            "deberta.encoder.layer.15.output.dense.bias True\n",
            "deberta.encoder.layer.15.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.15.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.16.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.16.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.16.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.16.attention.output.dense.weight True\n",
            "deberta.encoder.layer.16.attention.output.dense.bias True\n",
            "deberta.encoder.layer.16.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.16.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.16.intermediate.dense.weight True\n",
            "deberta.encoder.layer.16.intermediate.dense.bias True\n",
            "deberta.encoder.layer.16.output.dense.weight True\n",
            "deberta.encoder.layer.16.output.dense.bias True\n",
            "deberta.encoder.layer.16.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.16.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.17.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.17.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.17.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.17.attention.output.dense.weight True\n",
            "deberta.encoder.layer.17.attention.output.dense.bias True\n",
            "deberta.encoder.layer.17.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.17.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.17.intermediate.dense.weight True\n",
            "deberta.encoder.layer.17.intermediate.dense.bias True\n",
            "deberta.encoder.layer.17.output.dense.weight True\n",
            "deberta.encoder.layer.17.output.dense.bias True\n",
            "deberta.encoder.layer.17.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.17.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.18.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.18.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.18.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.18.attention.output.dense.weight True\n",
            "deberta.encoder.layer.18.attention.output.dense.bias True\n",
            "deberta.encoder.layer.18.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.18.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.18.intermediate.dense.weight True\n",
            "deberta.encoder.layer.18.intermediate.dense.bias True\n",
            "deberta.encoder.layer.18.output.dense.weight True\n",
            "deberta.encoder.layer.18.output.dense.bias True\n",
            "deberta.encoder.layer.18.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.18.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.19.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.19.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.19.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.19.attention.output.dense.weight True\n",
            "deberta.encoder.layer.19.attention.output.dense.bias True\n",
            "deberta.encoder.layer.19.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.19.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.19.intermediate.dense.weight True\n",
            "deberta.encoder.layer.19.intermediate.dense.bias True\n",
            "deberta.encoder.layer.19.output.dense.weight True\n",
            "deberta.encoder.layer.19.output.dense.bias True\n",
            "deberta.encoder.layer.19.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.19.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.20.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.20.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.20.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.20.attention.output.dense.weight True\n",
            "deberta.encoder.layer.20.attention.output.dense.bias True\n",
            "deberta.encoder.layer.20.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.20.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.20.intermediate.dense.weight True\n",
            "deberta.encoder.layer.20.intermediate.dense.bias True\n",
            "deberta.encoder.layer.20.output.dense.weight True\n",
            "deberta.encoder.layer.20.output.dense.bias True\n",
            "deberta.encoder.layer.20.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.20.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.21.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.21.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.21.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.21.attention.output.dense.weight True\n",
            "deberta.encoder.layer.21.attention.output.dense.bias True\n",
            "deberta.encoder.layer.21.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.21.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.21.intermediate.dense.weight True\n",
            "deberta.encoder.layer.21.intermediate.dense.bias True\n",
            "deberta.encoder.layer.21.output.dense.weight True\n",
            "deberta.encoder.layer.21.output.dense.bias True\n",
            "deberta.encoder.layer.21.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.21.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.22.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.22.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.22.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.22.attention.output.dense.weight True\n",
            "deberta.encoder.layer.22.attention.output.dense.bias True\n",
            "deberta.encoder.layer.22.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.22.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.22.intermediate.dense.weight True\n",
            "deberta.encoder.layer.22.intermediate.dense.bias True\n",
            "deberta.encoder.layer.22.output.dense.weight True\n",
            "deberta.encoder.layer.22.output.dense.bias True\n",
            "deberta.encoder.layer.22.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.22.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.23.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.23.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.23.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.23.attention.output.dense.weight True\n",
            "deberta.encoder.layer.23.attention.output.dense.bias True\n",
            "deberta.encoder.layer.23.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.23.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.23.intermediate.dense.weight True\n",
            "deberta.encoder.layer.23.intermediate.dense.bias True\n",
            "deberta.encoder.layer.23.output.dense.weight True\n",
            "deberta.encoder.layer.23.output.dense.bias True\n",
            "deberta.encoder.layer.23.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.23.output.LayerNorm.bias True\n",
            "deberta.encoder.rel_embeddings.weight True\n",
            "deberta.encoder.LayerNorm.weight True\n",
            "deberta.encoder.LayerNorm.bias True\n",
            "pooler.dense.weight True\n",
            "pooler.dense.bias True\n",
            "classifier.weight True\n",
            "classifier.bias True\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5100' max='5110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5100/5110 1:03:04 < 00:07, 1.35 it/s, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Content Rmse</th>\n",
              "      <th>Wording Rmse</th>\n",
              "      <th>Mcrmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.759219</td>\n",
              "      <td>0.787772</td>\n",
              "      <td>0.762141</td>\n",
              "      <td>0.774956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.666016</td>\n",
              "      <td>0.632377</td>\n",
              "      <td>0.729749</td>\n",
              "      <td>0.681063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.569413</td>\n",
              "      <td>0.575235</td>\n",
              "      <td>0.598637</td>\n",
              "      <td>0.586936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.521616</td>\n",
              "      <td>0.500598</td>\n",
              "      <td>0.572558</td>\n",
              "      <td>0.536578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.607600</td>\n",
              "      <td>0.457823</td>\n",
              "      <td>0.410337</td>\n",
              "      <td>0.532622</td>\n",
              "      <td>0.471480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.607600</td>\n",
              "      <td>0.488606</td>\n",
              "      <td>0.414557</td>\n",
              "      <td>0.590692</td>\n",
              "      <td>0.502625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.607600</td>\n",
              "      <td>0.500115</td>\n",
              "      <td>0.509321</td>\n",
              "      <td>0.521387</td>\n",
              "      <td>0.515354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.607600</td>\n",
              "      <td>0.459572</td>\n",
              "      <td>0.421155</td>\n",
              "      <td>0.524301</td>\n",
              "      <td>0.472728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.607600</td>\n",
              "      <td>0.520324</td>\n",
              "      <td>0.445163</td>\n",
              "      <td>0.620327</td>\n",
              "      <td>0.532745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.494900</td>\n",
              "      <td>0.496489</td>\n",
              "      <td>0.423983</td>\n",
              "      <td>0.597824</td>\n",
              "      <td>0.510903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.494900</td>\n",
              "      <td>0.457487</td>\n",
              "      <td>0.428646</td>\n",
              "      <td>0.519447</td>\n",
              "      <td>0.474046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.494900</td>\n",
              "      <td>0.475453</td>\n",
              "      <td>0.420535</td>\n",
              "      <td>0.556229</td>\n",
              "      <td>0.488382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.494900</td>\n",
              "      <td>0.491275</td>\n",
              "      <td>0.477860</td>\n",
              "      <td>0.529563</td>\n",
              "      <td>0.503711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.494900</td>\n",
              "      <td>0.479015</td>\n",
              "      <td>0.458481</td>\n",
              "      <td>0.525292</td>\n",
              "      <td>0.491887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.444400</td>\n",
              "      <td>0.477027</td>\n",
              "      <td>0.460154</td>\n",
              "      <td>0.518695</td>\n",
              "      <td>0.489425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.444400</td>\n",
              "      <td>0.455241</td>\n",
              "      <td>0.412404</td>\n",
              "      <td>0.526244</td>\n",
              "      <td>0.469324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.444400</td>\n",
              "      <td>0.616716</td>\n",
              "      <td>0.404563</td>\n",
              "      <td>0.853152</td>\n",
              "      <td>0.628857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.444400</td>\n",
              "      <td>0.507302</td>\n",
              "      <td>0.509371</td>\n",
              "      <td>0.531327</td>\n",
              "      <td>0.520349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.444400</td>\n",
              "      <td>0.504160</td>\n",
              "      <td>0.481000</td>\n",
              "      <td>0.554954</td>\n",
              "      <td>0.517977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.422800</td>\n",
              "      <td>0.514431</td>\n",
              "      <td>0.437555</td>\n",
              "      <td>0.617503</td>\n",
              "      <td>0.527529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.422800</td>\n",
              "      <td>0.488014</td>\n",
              "      <td>0.410159</td>\n",
              "      <td>0.591904</td>\n",
              "      <td>0.501031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.422800</td>\n",
              "      <td>0.493107</td>\n",
              "      <td>0.467558</td>\n",
              "      <td>0.543637</td>\n",
              "      <td>0.505597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.422800</td>\n",
              "      <td>0.516349</td>\n",
              "      <td>0.490480</td>\n",
              "      <td>0.567548</td>\n",
              "      <td>0.529014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.422800</td>\n",
              "      <td>0.480395</td>\n",
              "      <td>0.470362</td>\n",
              "      <td>0.518337</td>\n",
              "      <td>0.494350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.345600</td>\n",
              "      <td>0.517457</td>\n",
              "      <td>0.517736</td>\n",
              "      <td>0.542675</td>\n",
              "      <td>0.530205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.345600</td>\n",
              "      <td>0.501500</td>\n",
              "      <td>0.438527</td>\n",
              "      <td>0.591662</td>\n",
              "      <td>0.515095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.345600</td>\n",
              "      <td>0.490644</td>\n",
              "      <td>0.448417</td>\n",
              "      <td>0.560049</td>\n",
              "      <td>0.504233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.345600</td>\n",
              "      <td>0.508371</td>\n",
              "      <td>0.471672</td>\n",
              "      <td>0.570306</td>\n",
              "      <td>0.520989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.345600</td>\n",
              "      <td>0.531946</td>\n",
              "      <td>0.520896</td>\n",
              "      <td>0.568369</td>\n",
              "      <td>0.544632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.328100</td>\n",
              "      <td>0.499907</td>\n",
              "      <td>0.491621</td>\n",
              "      <td>0.534357</td>\n",
              "      <td>0.512989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.328100</td>\n",
              "      <td>0.518191</td>\n",
              "      <td>0.437324</td>\n",
              "      <td>0.624574</td>\n",
              "      <td>0.530949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.328100</td>\n",
              "      <td>0.508838</td>\n",
              "      <td>0.511880</td>\n",
              "      <td>0.532645</td>\n",
              "      <td>0.522263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.328100</td>\n",
              "      <td>0.524751</td>\n",
              "      <td>0.487365</td>\n",
              "      <td>0.588106</td>\n",
              "      <td>0.537736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.328100</td>\n",
              "      <td>0.525536</td>\n",
              "      <td>0.531935</td>\n",
              "      <td>0.544642</td>\n",
              "      <td>0.538288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.255700</td>\n",
              "      <td>0.492556</td>\n",
              "      <td>0.469794</td>\n",
              "      <td>0.541166</td>\n",
              "      <td>0.505480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.255700</td>\n",
              "      <td>0.505539</td>\n",
              "      <td>0.480501</td>\n",
              "      <td>0.556908</td>\n",
              "      <td>0.518705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.255700</td>\n",
              "      <td>0.515754</td>\n",
              "      <td>0.521780</td>\n",
              "      <td>0.535442</td>\n",
              "      <td>0.528611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.255700</td>\n",
              "      <td>0.500387</td>\n",
              "      <td>0.477349</td>\n",
              "      <td>0.549383</td>\n",
              "      <td>0.513366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.255700</td>\n",
              "      <td>0.491521</td>\n",
              "      <td>0.482703</td>\n",
              "      <td>0.527628</td>\n",
              "      <td>0.505166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.238800</td>\n",
              "      <td>0.514353</td>\n",
              "      <td>0.472352</td>\n",
              "      <td>0.583398</td>\n",
              "      <td>0.527875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.238800</td>\n",
              "      <td>0.545964</td>\n",
              "      <td>0.531378</td>\n",
              "      <td>0.585918</td>\n",
              "      <td>0.558648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.238800</td>\n",
              "      <td>0.545921</td>\n",
              "      <td>0.589789</td>\n",
              "      <td>0.529714</td>\n",
              "      <td>0.559752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.238800</td>\n",
              "      <td>0.514594</td>\n",
              "      <td>0.513603</td>\n",
              "      <td>0.542337</td>\n",
              "      <td>0.527970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.238800</td>\n",
              "      <td>0.531126</td>\n",
              "      <td>0.526461</td>\n",
              "      <td>0.561973</td>\n",
              "      <td>0.544217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.177600</td>\n",
              "      <td>0.514248</td>\n",
              "      <td>0.516533</td>\n",
              "      <td>0.537953</td>\n",
              "      <td>0.527243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.177600</td>\n",
              "      <td>0.524768</td>\n",
              "      <td>0.523473</td>\n",
              "      <td>0.552841</td>\n",
              "      <td>0.538157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.177600</td>\n",
              "      <td>0.518968</td>\n",
              "      <td>0.525397</td>\n",
              "      <td>0.539744</td>\n",
              "      <td>0.532570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.177600</td>\n",
              "      <td>0.527339</td>\n",
              "      <td>0.529474</td>\n",
              "      <td>0.551837</td>\n",
              "      <td>0.540656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.177600</td>\n",
              "      <td>0.520982</td>\n",
              "      <td>0.526792</td>\n",
              "      <td>0.541669</td>\n",
              "      <td>0.534230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.163500</td>\n",
              "      <td>0.520857</td>\n",
              "      <td>0.513519</td>\n",
              "      <td>0.554349</td>\n",
              "      <td>0.533934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.163500</td>\n",
              "      <td>0.523340</td>\n",
              "      <td>0.522973</td>\n",
              "      <td>0.549587</td>\n",
              "      <td>0.536280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold 1:\n",
            "deberta.embeddings.word_embeddings.weight False\n",
            "deberta.embeddings.LayerNorm.weight False\n",
            "deberta.embeddings.LayerNorm.bias False\n",
            "deberta.encoder.layer.0.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.0.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.0.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.0.attention.output.dense.weight False\n",
            "deberta.encoder.layer.0.attention.output.dense.bias False\n",
            "deberta.encoder.layer.0.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.0.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.0.intermediate.dense.weight False\n",
            "deberta.encoder.layer.0.intermediate.dense.bias False\n",
            "deberta.encoder.layer.0.output.dense.weight False\n",
            "deberta.encoder.layer.0.output.dense.bias False\n",
            "deberta.encoder.layer.0.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.0.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.1.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.1.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.1.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.1.attention.output.dense.weight False\n",
            "deberta.encoder.layer.1.attention.output.dense.bias False\n",
            "deberta.encoder.layer.1.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.1.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.1.intermediate.dense.weight False\n",
            "deberta.encoder.layer.1.intermediate.dense.bias False\n",
            "deberta.encoder.layer.1.output.dense.weight False\n",
            "deberta.encoder.layer.1.output.dense.bias False\n",
            "deberta.encoder.layer.1.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.1.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.2.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.2.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.2.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.2.attention.output.dense.weight False\n",
            "deberta.encoder.layer.2.attention.output.dense.bias False\n",
            "deberta.encoder.layer.2.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.2.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.2.intermediate.dense.weight False\n",
            "deberta.encoder.layer.2.intermediate.dense.bias False\n",
            "deberta.encoder.layer.2.output.dense.weight False\n",
            "deberta.encoder.layer.2.output.dense.bias False\n",
            "deberta.encoder.layer.2.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.2.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.3.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.3.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.3.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.3.attention.output.dense.weight False\n",
            "deberta.encoder.layer.3.attention.output.dense.bias False\n",
            "deberta.encoder.layer.3.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.3.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.3.intermediate.dense.weight False\n",
            "deberta.encoder.layer.3.intermediate.dense.bias False\n",
            "deberta.encoder.layer.3.output.dense.weight False\n",
            "deberta.encoder.layer.3.output.dense.bias False\n",
            "deberta.encoder.layer.3.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.3.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.4.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.4.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.4.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.4.attention.output.dense.weight False\n",
            "deberta.encoder.layer.4.attention.output.dense.bias False\n",
            "deberta.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.4.intermediate.dense.weight False\n",
            "deberta.encoder.layer.4.intermediate.dense.bias False\n",
            "deberta.encoder.layer.4.output.dense.weight False\n",
            "deberta.encoder.layer.4.output.dense.bias False\n",
            "deberta.encoder.layer.4.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.4.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.5.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.5.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.5.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.5.attention.output.dense.weight False\n",
            "deberta.encoder.layer.5.attention.output.dense.bias False\n",
            "deberta.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.5.intermediate.dense.weight False\n",
            "deberta.encoder.layer.5.intermediate.dense.bias False\n",
            "deberta.encoder.layer.5.output.dense.weight False\n",
            "deberta.encoder.layer.5.output.dense.bias False\n",
            "deberta.encoder.layer.5.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.5.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.6.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.6.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.6.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.6.attention.output.dense.weight True\n",
            "deberta.encoder.layer.6.attention.output.dense.bias True\n",
            "deberta.encoder.layer.6.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.6.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.6.intermediate.dense.weight True\n",
            "deberta.encoder.layer.6.intermediate.dense.bias True\n",
            "deberta.encoder.layer.6.output.dense.weight True\n",
            "deberta.encoder.layer.6.output.dense.bias True\n",
            "deberta.encoder.layer.6.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.6.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.7.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.7.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.7.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.7.attention.output.dense.weight True\n",
            "deberta.encoder.layer.7.attention.output.dense.bias True\n",
            "deberta.encoder.layer.7.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.7.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.7.intermediate.dense.weight True\n",
            "deberta.encoder.layer.7.intermediate.dense.bias True\n",
            "deberta.encoder.layer.7.output.dense.weight True\n",
            "deberta.encoder.layer.7.output.dense.bias True\n",
            "deberta.encoder.layer.7.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.7.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.8.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.8.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.8.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.8.attention.output.dense.weight True\n",
            "deberta.encoder.layer.8.attention.output.dense.bias True\n",
            "deberta.encoder.layer.8.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.8.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.8.intermediate.dense.weight True\n",
            "deberta.encoder.layer.8.intermediate.dense.bias True\n",
            "deberta.encoder.layer.8.output.dense.weight True\n",
            "deberta.encoder.layer.8.output.dense.bias True\n",
            "deberta.encoder.layer.8.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.8.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.9.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.9.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.9.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.9.attention.output.dense.weight True\n",
            "deberta.encoder.layer.9.attention.output.dense.bias True\n",
            "deberta.encoder.layer.9.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.9.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.9.intermediate.dense.weight True\n",
            "deberta.encoder.layer.9.intermediate.dense.bias True\n",
            "deberta.encoder.layer.9.output.dense.weight True\n",
            "deberta.encoder.layer.9.output.dense.bias True\n",
            "deberta.encoder.layer.9.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.9.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.10.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.10.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.10.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.10.attention.output.dense.weight True\n",
            "deberta.encoder.layer.10.attention.output.dense.bias True\n",
            "deberta.encoder.layer.10.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.10.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.10.intermediate.dense.weight True\n",
            "deberta.encoder.layer.10.intermediate.dense.bias True\n",
            "deberta.encoder.layer.10.output.dense.weight True\n",
            "deberta.encoder.layer.10.output.dense.bias True\n",
            "deberta.encoder.layer.10.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.10.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.11.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.11.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.11.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.11.attention.output.dense.weight True\n",
            "deberta.encoder.layer.11.attention.output.dense.bias True\n",
            "deberta.encoder.layer.11.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.11.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.11.intermediate.dense.weight True\n",
            "deberta.encoder.layer.11.intermediate.dense.bias True\n",
            "deberta.encoder.layer.11.output.dense.weight True\n",
            "deberta.encoder.layer.11.output.dense.bias True\n",
            "deberta.encoder.layer.11.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.11.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.12.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.12.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.12.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.12.attention.output.dense.weight True\n",
            "deberta.encoder.layer.12.attention.output.dense.bias True\n",
            "deberta.encoder.layer.12.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.12.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.12.intermediate.dense.weight True\n",
            "deberta.encoder.layer.12.intermediate.dense.bias True\n",
            "deberta.encoder.layer.12.output.dense.weight True\n",
            "deberta.encoder.layer.12.output.dense.bias True\n",
            "deberta.encoder.layer.12.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.12.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.13.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.13.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.13.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.13.attention.output.dense.weight True\n",
            "deberta.encoder.layer.13.attention.output.dense.bias True\n",
            "deberta.encoder.layer.13.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.13.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.13.intermediate.dense.weight True\n",
            "deberta.encoder.layer.13.intermediate.dense.bias True\n",
            "deberta.encoder.layer.13.output.dense.weight True\n",
            "deberta.encoder.layer.13.output.dense.bias True\n",
            "deberta.encoder.layer.13.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.13.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.14.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.14.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.14.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.14.attention.output.dense.weight True\n",
            "deberta.encoder.layer.14.attention.output.dense.bias True\n",
            "deberta.encoder.layer.14.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.14.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.14.intermediate.dense.weight True\n",
            "deberta.encoder.layer.14.intermediate.dense.bias True\n",
            "deberta.encoder.layer.14.output.dense.weight True\n",
            "deberta.encoder.layer.14.output.dense.bias True\n",
            "deberta.encoder.layer.14.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.14.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.15.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.15.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.15.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.15.attention.output.dense.weight True\n",
            "deberta.encoder.layer.15.attention.output.dense.bias True\n",
            "deberta.encoder.layer.15.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.15.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.15.intermediate.dense.weight True\n",
            "deberta.encoder.layer.15.intermediate.dense.bias True\n",
            "deberta.encoder.layer.15.output.dense.weight True\n",
            "deberta.encoder.layer.15.output.dense.bias True\n",
            "deberta.encoder.layer.15.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.15.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.16.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.16.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.16.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.16.attention.output.dense.weight True\n",
            "deberta.encoder.layer.16.attention.output.dense.bias True\n",
            "deberta.encoder.layer.16.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.16.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.16.intermediate.dense.weight True\n",
            "deberta.encoder.layer.16.intermediate.dense.bias True\n",
            "deberta.encoder.layer.16.output.dense.weight True\n",
            "deberta.encoder.layer.16.output.dense.bias True\n",
            "deberta.encoder.layer.16.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.16.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.17.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.17.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.17.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.17.attention.output.dense.weight True\n",
            "deberta.encoder.layer.17.attention.output.dense.bias True\n",
            "deberta.encoder.layer.17.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.17.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.17.intermediate.dense.weight True\n",
            "deberta.encoder.layer.17.intermediate.dense.bias True\n",
            "deberta.encoder.layer.17.output.dense.weight True\n",
            "deberta.encoder.layer.17.output.dense.bias True\n",
            "deberta.encoder.layer.17.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.17.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.18.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.18.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.18.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.18.attention.output.dense.weight True\n",
            "deberta.encoder.layer.18.attention.output.dense.bias True\n",
            "deberta.encoder.layer.18.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.18.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.18.intermediate.dense.weight True\n",
            "deberta.encoder.layer.18.intermediate.dense.bias True\n",
            "deberta.encoder.layer.18.output.dense.weight True\n",
            "deberta.encoder.layer.18.output.dense.bias True\n",
            "deberta.encoder.layer.18.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.18.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.19.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.19.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.19.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.19.attention.output.dense.weight True\n",
            "deberta.encoder.layer.19.attention.output.dense.bias True\n",
            "deberta.encoder.layer.19.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.19.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.19.intermediate.dense.weight True\n",
            "deberta.encoder.layer.19.intermediate.dense.bias True\n",
            "deberta.encoder.layer.19.output.dense.weight True\n",
            "deberta.encoder.layer.19.output.dense.bias True\n",
            "deberta.encoder.layer.19.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.19.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.20.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.20.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.20.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.20.attention.output.dense.weight True\n",
            "deberta.encoder.layer.20.attention.output.dense.bias True\n",
            "deberta.encoder.layer.20.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.20.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.20.intermediate.dense.weight True\n",
            "deberta.encoder.layer.20.intermediate.dense.bias True\n",
            "deberta.encoder.layer.20.output.dense.weight True\n",
            "deberta.encoder.layer.20.output.dense.bias True\n",
            "deberta.encoder.layer.20.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.20.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.21.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.21.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.21.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.21.attention.output.dense.weight True\n",
            "deberta.encoder.layer.21.attention.output.dense.bias True\n",
            "deberta.encoder.layer.21.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.21.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.21.intermediate.dense.weight True\n",
            "deberta.encoder.layer.21.intermediate.dense.bias True\n",
            "deberta.encoder.layer.21.output.dense.weight True\n",
            "deberta.encoder.layer.21.output.dense.bias True\n",
            "deberta.encoder.layer.21.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.21.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.22.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.22.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.22.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.22.attention.output.dense.weight True\n",
            "deberta.encoder.layer.22.attention.output.dense.bias True\n",
            "deberta.encoder.layer.22.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.22.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.22.intermediate.dense.weight True\n",
            "deberta.encoder.layer.22.intermediate.dense.bias True\n",
            "deberta.encoder.layer.22.output.dense.weight True\n",
            "deberta.encoder.layer.22.output.dense.bias True\n",
            "deberta.encoder.layer.22.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.22.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.23.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.23.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.23.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.23.attention.output.dense.weight True\n",
            "deberta.encoder.layer.23.attention.output.dense.bias True\n",
            "deberta.encoder.layer.23.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.23.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.23.intermediate.dense.weight True\n",
            "deberta.encoder.layer.23.intermediate.dense.bias True\n",
            "deberta.encoder.layer.23.output.dense.weight True\n",
            "deberta.encoder.layer.23.output.dense.bias True\n",
            "deberta.encoder.layer.23.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.23.output.LayerNorm.bias True\n",
            "deberta.encoder.rel_embeddings.weight True\n",
            "deberta.encoder.LayerNorm.weight True\n",
            "deberta.encoder.LayerNorm.bias True\n",
            "pooler.dense.weight True\n",
            "pooler.dense.bias True\n",
            "classifier.weight True\n",
            "classifier.bias True\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4700' max='5160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4700/5160 57:30 < 05:37, 1.36 it/s, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Content Rmse</th>\n",
              "      <th>Wording Rmse</th>\n",
              "      <th>Mcrmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.837719</td>\n",
              "      <td>0.831307</td>\n",
              "      <td>0.880622</td>\n",
              "      <td>0.855965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.944562</td>\n",
              "      <td>0.893945</td>\n",
              "      <td>1.037032</td>\n",
              "      <td>0.965488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.880874</td>\n",
              "      <td>0.792384</td>\n",
              "      <td>1.029770</td>\n",
              "      <td>0.911077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.668143</td>\n",
              "      <td>0.616966</td>\n",
              "      <td>0.766841</td>\n",
              "      <td>0.691903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.580200</td>\n",
              "      <td>0.891393</td>\n",
              "      <td>0.832154</td>\n",
              "      <td>1.008450</td>\n",
              "      <td>0.920302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.580200</td>\n",
              "      <td>0.708584</td>\n",
              "      <td>0.551116</td>\n",
              "      <td>0.935249</td>\n",
              "      <td>0.743183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.580200</td>\n",
              "      <td>0.680756</td>\n",
              "      <td>0.541622</td>\n",
              "      <td>0.888972</td>\n",
              "      <td>0.715297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.580200</td>\n",
              "      <td>0.671859</td>\n",
              "      <td>0.566346</td>\n",
              "      <td>0.835363</td>\n",
              "      <td>0.700854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.580200</td>\n",
              "      <td>0.811510</td>\n",
              "      <td>0.689029</td>\n",
              "      <td>1.001501</td>\n",
              "      <td>0.845265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.489500</td>\n",
              "      <td>0.739923</td>\n",
              "      <td>0.653220</td>\n",
              "      <td>0.889739</td>\n",
              "      <td>0.771480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.489500</td>\n",
              "      <td>0.687558</td>\n",
              "      <td>0.562093</td>\n",
              "      <td>0.875387</td>\n",
              "      <td>0.718740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.489500</td>\n",
              "      <td>0.650026</td>\n",
              "      <td>0.531347</td>\n",
              "      <td>0.833280</td>\n",
              "      <td>0.682313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.489500</td>\n",
              "      <td>0.749662</td>\n",
              "      <td>0.713934</td>\n",
              "      <td>0.850424</td>\n",
              "      <td>0.782179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.489500</td>\n",
              "      <td>0.658288</td>\n",
              "      <td>0.574585</td>\n",
              "      <td>0.800127</td>\n",
              "      <td>0.687356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.419700</td>\n",
              "      <td>0.728787</td>\n",
              "      <td>0.638468</td>\n",
              "      <td>0.886003</td>\n",
              "      <td>0.762235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.419700</td>\n",
              "      <td>0.806222</td>\n",
              "      <td>0.665820</td>\n",
              "      <td>1.041049</td>\n",
              "      <td>0.853434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.419700</td>\n",
              "      <td>0.744753</td>\n",
              "      <td>0.624144</td>\n",
              "      <td>0.937792</td>\n",
              "      <td>0.780968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.419700</td>\n",
              "      <td>0.719296</td>\n",
              "      <td>0.581290</td>\n",
              "      <td>0.936030</td>\n",
              "      <td>0.758660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.419700</td>\n",
              "      <td>0.668125</td>\n",
              "      <td>0.525011</td>\n",
              "      <td>0.891001</td>\n",
              "      <td>0.708006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.419500</td>\n",
              "      <td>0.859456</td>\n",
              "      <td>0.558543</td>\n",
              "      <td>1.249106</td>\n",
              "      <td>0.903824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.419500</td>\n",
              "      <td>0.811089</td>\n",
              "      <td>0.636785</td>\n",
              "      <td>1.072852</td>\n",
              "      <td>0.854819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.419500</td>\n",
              "      <td>0.657039</td>\n",
              "      <td>0.508163</td>\n",
              "      <td>0.881743</td>\n",
              "      <td>0.694953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.419500</td>\n",
              "      <td>0.864873</td>\n",
              "      <td>0.617686</td>\n",
              "      <td>1.194595</td>\n",
              "      <td>0.906140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.419500</td>\n",
              "      <td>0.708660</td>\n",
              "      <td>0.593907</td>\n",
              "      <td>0.895252</td>\n",
              "      <td>0.744580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.344300</td>\n",
              "      <td>0.704342</td>\n",
              "      <td>0.499525</td>\n",
              "      <td>0.981433</td>\n",
              "      <td>0.740479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.344300</td>\n",
              "      <td>0.676527</td>\n",
              "      <td>0.537099</td>\n",
              "      <td>0.893907</td>\n",
              "      <td>0.715503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.344300</td>\n",
              "      <td>0.671252</td>\n",
              "      <td>0.590832</td>\n",
              "      <td>0.817127</td>\n",
              "      <td>0.703979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.344300</td>\n",
              "      <td>0.714478</td>\n",
              "      <td>0.544119</td>\n",
              "      <td>0.964538</td>\n",
              "      <td>0.754328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.344300</td>\n",
              "      <td>0.652800</td>\n",
              "      <td>0.494248</td>\n",
              "      <td>0.884633</td>\n",
              "      <td>0.689441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.330500</td>\n",
              "      <td>0.740150</td>\n",
              "      <td>0.562654</td>\n",
              "      <td>1.002848</td>\n",
              "      <td>0.782751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.330500</td>\n",
              "      <td>0.736165</td>\n",
              "      <td>0.541753</td>\n",
              "      <td>1.020309</td>\n",
              "      <td>0.781031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.330500</td>\n",
              "      <td>0.695945</td>\n",
              "      <td>0.496533</td>\n",
              "      <td>0.961271</td>\n",
              "      <td>0.728902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.330500</td>\n",
              "      <td>0.721355</td>\n",
              "      <td>0.538703</td>\n",
              "      <td>0.981646</td>\n",
              "      <td>0.760174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.330500</td>\n",
              "      <td>0.688824</td>\n",
              "      <td>0.567302</td>\n",
              "      <td>0.879877</td>\n",
              "      <td>0.723589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.259600</td>\n",
              "      <td>0.691435</td>\n",
              "      <td>0.559505</td>\n",
              "      <td>0.897899</td>\n",
              "      <td>0.728702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.259600</td>\n",
              "      <td>0.658073</td>\n",
              "      <td>0.513379</td>\n",
              "      <td>0.880326</td>\n",
              "      <td>0.696852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.259600</td>\n",
              "      <td>0.736810</td>\n",
              "      <td>0.550554</td>\n",
              "      <td>1.006659</td>\n",
              "      <td>0.778606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.259600</td>\n",
              "      <td>0.697271</td>\n",
              "      <td>0.535573</td>\n",
              "      <td>0.938564</td>\n",
              "      <td>0.737069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.259600</td>\n",
              "      <td>0.738564</td>\n",
              "      <td>0.546148</td>\n",
              "      <td>1.007539</td>\n",
              "      <td>0.776843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.241000</td>\n",
              "      <td>0.728205</td>\n",
              "      <td>0.521301</td>\n",
              "      <td>1.013317</td>\n",
              "      <td>0.767309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.241000</td>\n",
              "      <td>0.721568</td>\n",
              "      <td>0.581470</td>\n",
              "      <td>0.940028</td>\n",
              "      <td>0.760749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.241000</td>\n",
              "      <td>0.675023</td>\n",
              "      <td>0.522136</td>\n",
              "      <td>0.911120</td>\n",
              "      <td>0.716628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.241000</td>\n",
              "      <td>0.668900</td>\n",
              "      <td>0.509370</td>\n",
              "      <td>0.902673</td>\n",
              "      <td>0.706022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.241000</td>\n",
              "      <td>0.723343</td>\n",
              "      <td>0.545632</td>\n",
              "      <td>0.984701</td>\n",
              "      <td>0.765166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.180900</td>\n",
              "      <td>0.693710</td>\n",
              "      <td>0.531491</td>\n",
              "      <td>0.933850</td>\n",
              "      <td>0.732670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.180900</td>\n",
              "      <td>0.726779</td>\n",
              "      <td>0.552579</td>\n",
              "      <td>0.982163</td>\n",
              "      <td>0.767371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.180900</td>\n",
              "      <td>0.716274</td>\n",
              "      <td>0.546153</td>\n",
              "      <td>0.964610</td>\n",
              "      <td>0.755382</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold 2:\n",
            "deberta.embeddings.word_embeddings.weight False\n",
            "deberta.embeddings.LayerNorm.weight False\n",
            "deberta.embeddings.LayerNorm.bias False\n",
            "deberta.encoder.layer.0.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.0.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.0.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.0.attention.output.dense.weight False\n",
            "deberta.encoder.layer.0.attention.output.dense.bias False\n",
            "deberta.encoder.layer.0.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.0.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.0.intermediate.dense.weight False\n",
            "deberta.encoder.layer.0.intermediate.dense.bias False\n",
            "deberta.encoder.layer.0.output.dense.weight False\n",
            "deberta.encoder.layer.0.output.dense.bias False\n",
            "deberta.encoder.layer.0.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.0.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.1.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.1.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.1.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.1.attention.output.dense.weight False\n",
            "deberta.encoder.layer.1.attention.output.dense.bias False\n",
            "deberta.encoder.layer.1.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.1.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.1.intermediate.dense.weight False\n",
            "deberta.encoder.layer.1.intermediate.dense.bias False\n",
            "deberta.encoder.layer.1.output.dense.weight False\n",
            "deberta.encoder.layer.1.output.dense.bias False\n",
            "deberta.encoder.layer.1.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.1.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.2.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.2.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.2.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.2.attention.output.dense.weight False\n",
            "deberta.encoder.layer.2.attention.output.dense.bias False\n",
            "deberta.encoder.layer.2.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.2.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.2.intermediate.dense.weight False\n",
            "deberta.encoder.layer.2.intermediate.dense.bias False\n",
            "deberta.encoder.layer.2.output.dense.weight False\n",
            "deberta.encoder.layer.2.output.dense.bias False\n",
            "deberta.encoder.layer.2.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.2.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.3.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.3.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.3.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.3.attention.output.dense.weight False\n",
            "deberta.encoder.layer.3.attention.output.dense.bias False\n",
            "deberta.encoder.layer.3.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.3.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.3.intermediate.dense.weight False\n",
            "deberta.encoder.layer.3.intermediate.dense.bias False\n",
            "deberta.encoder.layer.3.output.dense.weight False\n",
            "deberta.encoder.layer.3.output.dense.bias False\n",
            "deberta.encoder.layer.3.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.3.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.4.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.4.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.4.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.4.attention.output.dense.weight False\n",
            "deberta.encoder.layer.4.attention.output.dense.bias False\n",
            "deberta.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.4.intermediate.dense.weight False\n",
            "deberta.encoder.layer.4.intermediate.dense.bias False\n",
            "deberta.encoder.layer.4.output.dense.weight False\n",
            "deberta.encoder.layer.4.output.dense.bias False\n",
            "deberta.encoder.layer.4.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.4.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.5.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.5.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.5.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.5.attention.output.dense.weight False\n",
            "deberta.encoder.layer.5.attention.output.dense.bias False\n",
            "deberta.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.5.intermediate.dense.weight False\n",
            "deberta.encoder.layer.5.intermediate.dense.bias False\n",
            "deberta.encoder.layer.5.output.dense.weight False\n",
            "deberta.encoder.layer.5.output.dense.bias False\n",
            "deberta.encoder.layer.5.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.5.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.6.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.6.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.6.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.6.attention.output.dense.weight True\n",
            "deberta.encoder.layer.6.attention.output.dense.bias True\n",
            "deberta.encoder.layer.6.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.6.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.6.intermediate.dense.weight True\n",
            "deberta.encoder.layer.6.intermediate.dense.bias True\n",
            "deberta.encoder.layer.6.output.dense.weight True\n",
            "deberta.encoder.layer.6.output.dense.bias True\n",
            "deberta.encoder.layer.6.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.6.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.7.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.7.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.7.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.7.attention.output.dense.weight True\n",
            "deberta.encoder.layer.7.attention.output.dense.bias True\n",
            "deberta.encoder.layer.7.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.7.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.7.intermediate.dense.weight True\n",
            "deberta.encoder.layer.7.intermediate.dense.bias True\n",
            "deberta.encoder.layer.7.output.dense.weight True\n",
            "deberta.encoder.layer.7.output.dense.bias True\n",
            "deberta.encoder.layer.7.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.7.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.8.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.8.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.8.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.8.attention.output.dense.weight True\n",
            "deberta.encoder.layer.8.attention.output.dense.bias True\n",
            "deberta.encoder.layer.8.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.8.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.8.intermediate.dense.weight True\n",
            "deberta.encoder.layer.8.intermediate.dense.bias True\n",
            "deberta.encoder.layer.8.output.dense.weight True\n",
            "deberta.encoder.layer.8.output.dense.bias True\n",
            "deberta.encoder.layer.8.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.8.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.9.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.9.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.9.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.9.attention.output.dense.weight True\n",
            "deberta.encoder.layer.9.attention.output.dense.bias True\n",
            "deberta.encoder.layer.9.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.9.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.9.intermediate.dense.weight True\n",
            "deberta.encoder.layer.9.intermediate.dense.bias True\n",
            "deberta.encoder.layer.9.output.dense.weight True\n",
            "deberta.encoder.layer.9.output.dense.bias True\n",
            "deberta.encoder.layer.9.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.9.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.10.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.10.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.10.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.10.attention.output.dense.weight True\n",
            "deberta.encoder.layer.10.attention.output.dense.bias True\n",
            "deberta.encoder.layer.10.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.10.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.10.intermediate.dense.weight True\n",
            "deberta.encoder.layer.10.intermediate.dense.bias True\n",
            "deberta.encoder.layer.10.output.dense.weight True\n",
            "deberta.encoder.layer.10.output.dense.bias True\n",
            "deberta.encoder.layer.10.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.10.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.11.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.11.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.11.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.11.attention.output.dense.weight True\n",
            "deberta.encoder.layer.11.attention.output.dense.bias True\n",
            "deberta.encoder.layer.11.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.11.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.11.intermediate.dense.weight True\n",
            "deberta.encoder.layer.11.intermediate.dense.bias True\n",
            "deberta.encoder.layer.11.output.dense.weight True\n",
            "deberta.encoder.layer.11.output.dense.bias True\n",
            "deberta.encoder.layer.11.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.11.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.12.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.12.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.12.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.12.attention.output.dense.weight True\n",
            "deberta.encoder.layer.12.attention.output.dense.bias True\n",
            "deberta.encoder.layer.12.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.12.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.12.intermediate.dense.weight True\n",
            "deberta.encoder.layer.12.intermediate.dense.bias True\n",
            "deberta.encoder.layer.12.output.dense.weight True\n",
            "deberta.encoder.layer.12.output.dense.bias True\n",
            "deberta.encoder.layer.12.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.12.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.13.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.13.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.13.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.13.attention.output.dense.weight True\n",
            "deberta.encoder.layer.13.attention.output.dense.bias True\n",
            "deberta.encoder.layer.13.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.13.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.13.intermediate.dense.weight True\n",
            "deberta.encoder.layer.13.intermediate.dense.bias True\n",
            "deberta.encoder.layer.13.output.dense.weight True\n",
            "deberta.encoder.layer.13.output.dense.bias True\n",
            "deberta.encoder.layer.13.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.13.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.14.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.14.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.14.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.14.attention.output.dense.weight True\n",
            "deberta.encoder.layer.14.attention.output.dense.bias True\n",
            "deberta.encoder.layer.14.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.14.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.14.intermediate.dense.weight True\n",
            "deberta.encoder.layer.14.intermediate.dense.bias True\n",
            "deberta.encoder.layer.14.output.dense.weight True\n",
            "deberta.encoder.layer.14.output.dense.bias True\n",
            "deberta.encoder.layer.14.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.14.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.15.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.15.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.15.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.15.attention.output.dense.weight True\n",
            "deberta.encoder.layer.15.attention.output.dense.bias True\n",
            "deberta.encoder.layer.15.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.15.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.15.intermediate.dense.weight True\n",
            "deberta.encoder.layer.15.intermediate.dense.bias True\n",
            "deberta.encoder.layer.15.output.dense.weight True\n",
            "deberta.encoder.layer.15.output.dense.bias True\n",
            "deberta.encoder.layer.15.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.15.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.16.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.16.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.16.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.16.attention.output.dense.weight True\n",
            "deberta.encoder.layer.16.attention.output.dense.bias True\n",
            "deberta.encoder.layer.16.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.16.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.16.intermediate.dense.weight True\n",
            "deberta.encoder.layer.16.intermediate.dense.bias True\n",
            "deberta.encoder.layer.16.output.dense.weight True\n",
            "deberta.encoder.layer.16.output.dense.bias True\n",
            "deberta.encoder.layer.16.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.16.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.17.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.17.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.17.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.17.attention.output.dense.weight True\n",
            "deberta.encoder.layer.17.attention.output.dense.bias True\n",
            "deberta.encoder.layer.17.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.17.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.17.intermediate.dense.weight True\n",
            "deberta.encoder.layer.17.intermediate.dense.bias True\n",
            "deberta.encoder.layer.17.output.dense.weight True\n",
            "deberta.encoder.layer.17.output.dense.bias True\n",
            "deberta.encoder.layer.17.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.17.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.18.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.18.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.18.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.18.attention.output.dense.weight True\n",
            "deberta.encoder.layer.18.attention.output.dense.bias True\n",
            "deberta.encoder.layer.18.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.18.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.18.intermediate.dense.weight True\n",
            "deberta.encoder.layer.18.intermediate.dense.bias True\n",
            "deberta.encoder.layer.18.output.dense.weight True\n",
            "deberta.encoder.layer.18.output.dense.bias True\n",
            "deberta.encoder.layer.18.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.18.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.19.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.19.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.19.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.19.attention.output.dense.weight True\n",
            "deberta.encoder.layer.19.attention.output.dense.bias True\n",
            "deberta.encoder.layer.19.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.19.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.19.intermediate.dense.weight True\n",
            "deberta.encoder.layer.19.intermediate.dense.bias True\n",
            "deberta.encoder.layer.19.output.dense.weight True\n",
            "deberta.encoder.layer.19.output.dense.bias True\n",
            "deberta.encoder.layer.19.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.19.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.20.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.20.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.20.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.20.attention.output.dense.weight True\n",
            "deberta.encoder.layer.20.attention.output.dense.bias True\n",
            "deberta.encoder.layer.20.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.20.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.20.intermediate.dense.weight True\n",
            "deberta.encoder.layer.20.intermediate.dense.bias True\n",
            "deberta.encoder.layer.20.output.dense.weight True\n",
            "deberta.encoder.layer.20.output.dense.bias True\n",
            "deberta.encoder.layer.20.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.20.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.21.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.21.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.21.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.21.attention.output.dense.weight True\n",
            "deberta.encoder.layer.21.attention.output.dense.bias True\n",
            "deberta.encoder.layer.21.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.21.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.21.intermediate.dense.weight True\n",
            "deberta.encoder.layer.21.intermediate.dense.bias True\n",
            "deberta.encoder.layer.21.output.dense.weight True\n",
            "deberta.encoder.layer.21.output.dense.bias True\n",
            "deberta.encoder.layer.21.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.21.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.22.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.22.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.22.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.22.attention.output.dense.weight True\n",
            "deberta.encoder.layer.22.attention.output.dense.bias True\n",
            "deberta.encoder.layer.22.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.22.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.22.intermediate.dense.weight True\n",
            "deberta.encoder.layer.22.intermediate.dense.bias True\n",
            "deberta.encoder.layer.22.output.dense.weight True\n",
            "deberta.encoder.layer.22.output.dense.bias True\n",
            "deberta.encoder.layer.22.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.22.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.23.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.23.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.23.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.23.attention.output.dense.weight True\n",
            "deberta.encoder.layer.23.attention.output.dense.bias True\n",
            "deberta.encoder.layer.23.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.23.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.23.intermediate.dense.weight True\n",
            "deberta.encoder.layer.23.intermediate.dense.bias True\n",
            "deberta.encoder.layer.23.output.dense.weight True\n",
            "deberta.encoder.layer.23.output.dense.bias True\n",
            "deberta.encoder.layer.23.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.23.output.LayerNorm.bias True\n",
            "deberta.encoder.rel_embeddings.weight True\n",
            "deberta.encoder.LayerNorm.weight True\n",
            "deberta.encoder.LayerNorm.bias True\n",
            "pooler.dense.weight True\n",
            "pooler.dense.bias True\n",
            "classifier.weight True\n",
            "classifier.bias True\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5170' max='5170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5170/5170 1:03:02, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Content Rmse</th>\n",
              "      <th>Wording Rmse</th>\n",
              "      <th>Mcrmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.582203</td>\n",
              "      <td>0.580814</td>\n",
              "      <td>0.617328</td>\n",
              "      <td>0.599071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.606620</td>\n",
              "      <td>0.494394</td>\n",
              "      <td>0.751267</td>\n",
              "      <td>0.622831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.602472</td>\n",
              "      <td>0.595484</td>\n",
              "      <td>0.635750</td>\n",
              "      <td>0.615617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.499979</td>\n",
              "      <td>0.496317</td>\n",
              "      <td>0.536321</td>\n",
              "      <td>0.516319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.591900</td>\n",
              "      <td>0.479097</td>\n",
              "      <td>0.430395</td>\n",
              "      <td>0.556901</td>\n",
              "      <td>0.493648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.591900</td>\n",
              "      <td>0.467174</td>\n",
              "      <td>0.443633</td>\n",
              "      <td>0.520188</td>\n",
              "      <td>0.481910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.591900</td>\n",
              "      <td>0.494703</td>\n",
              "      <td>0.440496</td>\n",
              "      <td>0.579908</td>\n",
              "      <td>0.510202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.591900</td>\n",
              "      <td>0.567375</td>\n",
              "      <td>0.631065</td>\n",
              "      <td>0.532054</td>\n",
              "      <td>0.581560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.591900</td>\n",
              "      <td>0.569502</td>\n",
              "      <td>0.536409</td>\n",
              "      <td>0.635999</td>\n",
              "      <td>0.586204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.494500</td>\n",
              "      <td>0.475091</td>\n",
              "      <td>0.456596</td>\n",
              "      <td>0.520141</td>\n",
              "      <td>0.488369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.494500</td>\n",
              "      <td>0.463141</td>\n",
              "      <td>0.417368</td>\n",
              "      <td>0.534827</td>\n",
              "      <td>0.476098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.494500</td>\n",
              "      <td>0.510769</td>\n",
              "      <td>0.507971</td>\n",
              "      <td>0.548850</td>\n",
              "      <td>0.528410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.494500</td>\n",
              "      <td>0.475330</td>\n",
              "      <td>0.447606</td>\n",
              "      <td>0.531276</td>\n",
              "      <td>0.489441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.494500</td>\n",
              "      <td>0.494202</td>\n",
              "      <td>0.439561</td>\n",
              "      <td>0.578336</td>\n",
              "      <td>0.508948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.431400</td>\n",
              "      <td>0.479313</td>\n",
              "      <td>0.438503</td>\n",
              "      <td>0.550064</td>\n",
              "      <td>0.494283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.431400</td>\n",
              "      <td>0.456615</td>\n",
              "      <td>0.408153</td>\n",
              "      <td>0.534071</td>\n",
              "      <td>0.471112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.431400</td>\n",
              "      <td>0.506028</td>\n",
              "      <td>0.507597</td>\n",
              "      <td>0.538097</td>\n",
              "      <td>0.522847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.431400</td>\n",
              "      <td>0.469755</td>\n",
              "      <td>0.425787</td>\n",
              "      <td>0.542253</td>\n",
              "      <td>0.484020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.431400</td>\n",
              "      <td>0.478912</td>\n",
              "      <td>0.481300</td>\n",
              "      <td>0.503942</td>\n",
              "      <td>0.492621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.417400</td>\n",
              "      <td>0.540136</td>\n",
              "      <td>0.521825</td>\n",
              "      <td>0.588728</td>\n",
              "      <td>0.555276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.417400</td>\n",
              "      <td>0.446103</td>\n",
              "      <td>0.412952</td>\n",
              "      <td>0.508915</td>\n",
              "      <td>0.460933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.417400</td>\n",
              "      <td>0.584407</td>\n",
              "      <td>0.449775</td>\n",
              "      <td>0.753983</td>\n",
              "      <td>0.601879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.417400</td>\n",
              "      <td>0.477001</td>\n",
              "      <td>0.458528</td>\n",
              "      <td>0.528653</td>\n",
              "      <td>0.493590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.417400</td>\n",
              "      <td>0.498040</td>\n",
              "      <td>0.428482</td>\n",
              "      <td>0.596672</td>\n",
              "      <td>0.512577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.493947</td>\n",
              "      <td>0.420394</td>\n",
              "      <td>0.596548</td>\n",
              "      <td>0.508471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.486667</td>\n",
              "      <td>0.419530</td>\n",
              "      <td>0.584078</td>\n",
              "      <td>0.501804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.475898</td>\n",
              "      <td>0.420821</td>\n",
              "      <td>0.558887</td>\n",
              "      <td>0.489854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.457250</td>\n",
              "      <td>0.419021</td>\n",
              "      <td>0.523870</td>\n",
              "      <td>0.471446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.498538</td>\n",
              "      <td>0.419166</td>\n",
              "      <td>0.605812</td>\n",
              "      <td>0.512489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.316800</td>\n",
              "      <td>0.479880</td>\n",
              "      <td>0.415933</td>\n",
              "      <td>0.571382</td>\n",
              "      <td>0.493658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.316800</td>\n",
              "      <td>0.468552</td>\n",
              "      <td>0.434693</td>\n",
              "      <td>0.530673</td>\n",
              "      <td>0.482683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.316800</td>\n",
              "      <td>0.470456</td>\n",
              "      <td>0.417654</td>\n",
              "      <td>0.551567</td>\n",
              "      <td>0.484611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.316800</td>\n",
              "      <td>0.471718</td>\n",
              "      <td>0.419991</td>\n",
              "      <td>0.552722</td>\n",
              "      <td>0.486356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.316800</td>\n",
              "      <td>0.500207</td>\n",
              "      <td>0.421666</td>\n",
              "      <td>0.604678</td>\n",
              "      <td>0.513172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.250600</td>\n",
              "      <td>0.485552</td>\n",
              "      <td>0.421836</td>\n",
              "      <td>0.577867</td>\n",
              "      <td>0.499852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.250600</td>\n",
              "      <td>0.496027</td>\n",
              "      <td>0.439200</td>\n",
              "      <td>0.581654</td>\n",
              "      <td>0.510427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.250600</td>\n",
              "      <td>0.505507</td>\n",
              "      <td>0.429544</td>\n",
              "      <td>0.610198</td>\n",
              "      <td>0.519871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.250600</td>\n",
              "      <td>0.506474</td>\n",
              "      <td>0.467679</td>\n",
              "      <td>0.572601</td>\n",
              "      <td>0.520140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.250600</td>\n",
              "      <td>0.478604</td>\n",
              "      <td>0.431163</td>\n",
              "      <td>0.553629</td>\n",
              "      <td>0.492396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.224400</td>\n",
              "      <td>0.495859</td>\n",
              "      <td>0.441687</td>\n",
              "      <td>0.577762</td>\n",
              "      <td>0.509725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.224400</td>\n",
              "      <td>0.496925</td>\n",
              "      <td>0.418852</td>\n",
              "      <td>0.602758</td>\n",
              "      <td>0.510805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.224400</td>\n",
              "      <td>0.494283</td>\n",
              "      <td>0.452205</td>\n",
              "      <td>0.564030</td>\n",
              "      <td>0.508117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.224400</td>\n",
              "      <td>0.486703</td>\n",
              "      <td>0.437641</td>\n",
              "      <td>0.562761</td>\n",
              "      <td>0.500201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.224400</td>\n",
              "      <td>0.480535</td>\n",
              "      <td>0.438390</td>\n",
              "      <td>0.550040</td>\n",
              "      <td>0.494215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.169300</td>\n",
              "      <td>0.488599</td>\n",
              "      <td>0.440637</td>\n",
              "      <td>0.563848</td>\n",
              "      <td>0.502242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.169300</td>\n",
              "      <td>0.492146</td>\n",
              "      <td>0.437806</td>\n",
              "      <td>0.573713</td>\n",
              "      <td>0.505759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.169300</td>\n",
              "      <td>0.492181</td>\n",
              "      <td>0.429499</td>\n",
              "      <td>0.582939</td>\n",
              "      <td>0.506219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.169300</td>\n",
              "      <td>0.488717</td>\n",
              "      <td>0.437470</td>\n",
              "      <td>0.567088</td>\n",
              "      <td>0.502279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.169300</td>\n",
              "      <td>0.491092</td>\n",
              "      <td>0.433892</td>\n",
              "      <td>0.576076</td>\n",
              "      <td>0.504984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.152300</td>\n",
              "      <td>0.491120</td>\n",
              "      <td>0.448160</td>\n",
              "      <td>0.561228</td>\n",
              "      <td>0.504694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.152300</td>\n",
              "      <td>0.490750</td>\n",
              "      <td>0.446512</td>\n",
              "      <td>0.561888</td>\n",
              "      <td>0.504200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold 3:\n",
            "deberta.embeddings.word_embeddings.weight False\n",
            "deberta.embeddings.LayerNorm.weight False\n",
            "deberta.embeddings.LayerNorm.bias False\n",
            "deberta.encoder.layer.0.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.0.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.0.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.0.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.0.attention.output.dense.weight False\n",
            "deberta.encoder.layer.0.attention.output.dense.bias False\n",
            "deberta.encoder.layer.0.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.0.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.0.intermediate.dense.weight False\n",
            "deberta.encoder.layer.0.intermediate.dense.bias False\n",
            "deberta.encoder.layer.0.output.dense.weight False\n",
            "deberta.encoder.layer.0.output.dense.bias False\n",
            "deberta.encoder.layer.0.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.0.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.1.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.1.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.1.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.1.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.1.attention.output.dense.weight False\n",
            "deberta.encoder.layer.1.attention.output.dense.bias False\n",
            "deberta.encoder.layer.1.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.1.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.1.intermediate.dense.weight False\n",
            "deberta.encoder.layer.1.intermediate.dense.bias False\n",
            "deberta.encoder.layer.1.output.dense.weight False\n",
            "deberta.encoder.layer.1.output.dense.bias False\n",
            "deberta.encoder.layer.1.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.1.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.2.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.2.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.2.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.2.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.2.attention.output.dense.weight False\n",
            "deberta.encoder.layer.2.attention.output.dense.bias False\n",
            "deberta.encoder.layer.2.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.2.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.2.intermediate.dense.weight False\n",
            "deberta.encoder.layer.2.intermediate.dense.bias False\n",
            "deberta.encoder.layer.2.output.dense.weight False\n",
            "deberta.encoder.layer.2.output.dense.bias False\n",
            "deberta.encoder.layer.2.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.2.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.3.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.3.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.3.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.3.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.3.attention.output.dense.weight False\n",
            "deberta.encoder.layer.3.attention.output.dense.bias False\n",
            "deberta.encoder.layer.3.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.3.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.3.intermediate.dense.weight False\n",
            "deberta.encoder.layer.3.intermediate.dense.bias False\n",
            "deberta.encoder.layer.3.output.dense.weight False\n",
            "deberta.encoder.layer.3.output.dense.bias False\n",
            "deberta.encoder.layer.3.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.3.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.4.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.4.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.4.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.4.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.4.attention.output.dense.weight False\n",
            "deberta.encoder.layer.4.attention.output.dense.bias False\n",
            "deberta.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.4.intermediate.dense.weight False\n",
            "deberta.encoder.layer.4.intermediate.dense.bias False\n",
            "deberta.encoder.layer.4.output.dense.weight False\n",
            "deberta.encoder.layer.4.output.dense.bias False\n",
            "deberta.encoder.layer.4.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.4.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.5.attention.self.query_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.query_proj.bias False\n",
            "deberta.encoder.layer.5.attention.self.key_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.key_proj.bias False\n",
            "deberta.encoder.layer.5.attention.self.value_proj.weight False\n",
            "deberta.encoder.layer.5.attention.self.value_proj.bias False\n",
            "deberta.encoder.layer.5.attention.output.dense.weight False\n",
            "deberta.encoder.layer.5.attention.output.dense.bias False\n",
            "deberta.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.5.intermediate.dense.weight False\n",
            "deberta.encoder.layer.5.intermediate.dense.bias False\n",
            "deberta.encoder.layer.5.output.dense.weight False\n",
            "deberta.encoder.layer.5.output.dense.bias False\n",
            "deberta.encoder.layer.5.output.LayerNorm.weight False\n",
            "deberta.encoder.layer.5.output.LayerNorm.bias False\n",
            "deberta.encoder.layer.6.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.6.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.6.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.6.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.6.attention.output.dense.weight True\n",
            "deberta.encoder.layer.6.attention.output.dense.bias True\n",
            "deberta.encoder.layer.6.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.6.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.6.intermediate.dense.weight True\n",
            "deberta.encoder.layer.6.intermediate.dense.bias True\n",
            "deberta.encoder.layer.6.output.dense.weight True\n",
            "deberta.encoder.layer.6.output.dense.bias True\n",
            "deberta.encoder.layer.6.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.6.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.7.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.7.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.7.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.7.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.7.attention.output.dense.weight True\n",
            "deberta.encoder.layer.7.attention.output.dense.bias True\n",
            "deberta.encoder.layer.7.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.7.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.7.intermediate.dense.weight True\n",
            "deberta.encoder.layer.7.intermediate.dense.bias True\n",
            "deberta.encoder.layer.7.output.dense.weight True\n",
            "deberta.encoder.layer.7.output.dense.bias True\n",
            "deberta.encoder.layer.7.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.7.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.8.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.8.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.8.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.8.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.8.attention.output.dense.weight True\n",
            "deberta.encoder.layer.8.attention.output.dense.bias True\n",
            "deberta.encoder.layer.8.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.8.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.8.intermediate.dense.weight True\n",
            "deberta.encoder.layer.8.intermediate.dense.bias True\n",
            "deberta.encoder.layer.8.output.dense.weight True\n",
            "deberta.encoder.layer.8.output.dense.bias True\n",
            "deberta.encoder.layer.8.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.8.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.9.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.9.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.9.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.9.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.9.attention.output.dense.weight True\n",
            "deberta.encoder.layer.9.attention.output.dense.bias True\n",
            "deberta.encoder.layer.9.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.9.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.9.intermediate.dense.weight True\n",
            "deberta.encoder.layer.9.intermediate.dense.bias True\n",
            "deberta.encoder.layer.9.output.dense.weight True\n",
            "deberta.encoder.layer.9.output.dense.bias True\n",
            "deberta.encoder.layer.9.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.9.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.10.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.10.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.10.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.10.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.10.attention.output.dense.weight True\n",
            "deberta.encoder.layer.10.attention.output.dense.bias True\n",
            "deberta.encoder.layer.10.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.10.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.10.intermediate.dense.weight True\n",
            "deberta.encoder.layer.10.intermediate.dense.bias True\n",
            "deberta.encoder.layer.10.output.dense.weight True\n",
            "deberta.encoder.layer.10.output.dense.bias True\n",
            "deberta.encoder.layer.10.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.10.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.11.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.11.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.11.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.11.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.11.attention.output.dense.weight True\n",
            "deberta.encoder.layer.11.attention.output.dense.bias True\n",
            "deberta.encoder.layer.11.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.11.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.11.intermediate.dense.weight True\n",
            "deberta.encoder.layer.11.intermediate.dense.bias True\n",
            "deberta.encoder.layer.11.output.dense.weight True\n",
            "deberta.encoder.layer.11.output.dense.bias True\n",
            "deberta.encoder.layer.11.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.11.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.12.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.12.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.12.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.12.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.12.attention.output.dense.weight True\n",
            "deberta.encoder.layer.12.attention.output.dense.bias True\n",
            "deberta.encoder.layer.12.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.12.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.12.intermediate.dense.weight True\n",
            "deberta.encoder.layer.12.intermediate.dense.bias True\n",
            "deberta.encoder.layer.12.output.dense.weight True\n",
            "deberta.encoder.layer.12.output.dense.bias True\n",
            "deberta.encoder.layer.12.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.12.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.13.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.13.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.13.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.13.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.13.attention.output.dense.weight True\n",
            "deberta.encoder.layer.13.attention.output.dense.bias True\n",
            "deberta.encoder.layer.13.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.13.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.13.intermediate.dense.weight True\n",
            "deberta.encoder.layer.13.intermediate.dense.bias True\n",
            "deberta.encoder.layer.13.output.dense.weight True\n",
            "deberta.encoder.layer.13.output.dense.bias True\n",
            "deberta.encoder.layer.13.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.13.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.14.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.14.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.14.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.14.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.14.attention.output.dense.weight True\n",
            "deberta.encoder.layer.14.attention.output.dense.bias True\n",
            "deberta.encoder.layer.14.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.14.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.14.intermediate.dense.weight True\n",
            "deberta.encoder.layer.14.intermediate.dense.bias True\n",
            "deberta.encoder.layer.14.output.dense.weight True\n",
            "deberta.encoder.layer.14.output.dense.bias True\n",
            "deberta.encoder.layer.14.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.14.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.15.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.15.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.15.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.15.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.15.attention.output.dense.weight True\n",
            "deberta.encoder.layer.15.attention.output.dense.bias True\n",
            "deberta.encoder.layer.15.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.15.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.15.intermediate.dense.weight True\n",
            "deberta.encoder.layer.15.intermediate.dense.bias True\n",
            "deberta.encoder.layer.15.output.dense.weight True\n",
            "deberta.encoder.layer.15.output.dense.bias True\n",
            "deberta.encoder.layer.15.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.15.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.16.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.16.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.16.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.16.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.16.attention.output.dense.weight True\n",
            "deberta.encoder.layer.16.attention.output.dense.bias True\n",
            "deberta.encoder.layer.16.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.16.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.16.intermediate.dense.weight True\n",
            "deberta.encoder.layer.16.intermediate.dense.bias True\n",
            "deberta.encoder.layer.16.output.dense.weight True\n",
            "deberta.encoder.layer.16.output.dense.bias True\n",
            "deberta.encoder.layer.16.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.16.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.17.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.17.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.17.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.17.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.17.attention.output.dense.weight True\n",
            "deberta.encoder.layer.17.attention.output.dense.bias True\n",
            "deberta.encoder.layer.17.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.17.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.17.intermediate.dense.weight True\n",
            "deberta.encoder.layer.17.intermediate.dense.bias True\n",
            "deberta.encoder.layer.17.output.dense.weight True\n",
            "deberta.encoder.layer.17.output.dense.bias True\n",
            "deberta.encoder.layer.17.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.17.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.18.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.18.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.18.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.18.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.18.attention.output.dense.weight True\n",
            "deberta.encoder.layer.18.attention.output.dense.bias True\n",
            "deberta.encoder.layer.18.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.18.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.18.intermediate.dense.weight True\n",
            "deberta.encoder.layer.18.intermediate.dense.bias True\n",
            "deberta.encoder.layer.18.output.dense.weight True\n",
            "deberta.encoder.layer.18.output.dense.bias True\n",
            "deberta.encoder.layer.18.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.18.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.19.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.19.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.19.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.19.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.19.attention.output.dense.weight True\n",
            "deberta.encoder.layer.19.attention.output.dense.bias True\n",
            "deberta.encoder.layer.19.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.19.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.19.intermediate.dense.weight True\n",
            "deberta.encoder.layer.19.intermediate.dense.bias True\n",
            "deberta.encoder.layer.19.output.dense.weight True\n",
            "deberta.encoder.layer.19.output.dense.bias True\n",
            "deberta.encoder.layer.19.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.19.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.20.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.20.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.20.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.20.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.20.attention.output.dense.weight True\n",
            "deberta.encoder.layer.20.attention.output.dense.bias True\n",
            "deberta.encoder.layer.20.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.20.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.20.intermediate.dense.weight True\n",
            "deberta.encoder.layer.20.intermediate.dense.bias True\n",
            "deberta.encoder.layer.20.output.dense.weight True\n",
            "deberta.encoder.layer.20.output.dense.bias True\n",
            "deberta.encoder.layer.20.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.20.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.21.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.21.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.21.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.21.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.21.attention.output.dense.weight True\n",
            "deberta.encoder.layer.21.attention.output.dense.bias True\n",
            "deberta.encoder.layer.21.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.21.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.21.intermediate.dense.weight True\n",
            "deberta.encoder.layer.21.intermediate.dense.bias True\n",
            "deberta.encoder.layer.21.output.dense.weight True\n",
            "deberta.encoder.layer.21.output.dense.bias True\n",
            "deberta.encoder.layer.21.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.21.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.22.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.22.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.22.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.22.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.22.attention.output.dense.weight True\n",
            "deberta.encoder.layer.22.attention.output.dense.bias True\n",
            "deberta.encoder.layer.22.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.22.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.22.intermediate.dense.weight True\n",
            "deberta.encoder.layer.22.intermediate.dense.bias True\n",
            "deberta.encoder.layer.22.output.dense.weight True\n",
            "deberta.encoder.layer.22.output.dense.bias True\n",
            "deberta.encoder.layer.22.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.22.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.23.attention.self.query_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.query_proj.bias True\n",
            "deberta.encoder.layer.23.attention.self.key_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.key_proj.bias True\n",
            "deberta.encoder.layer.23.attention.self.value_proj.weight True\n",
            "deberta.encoder.layer.23.attention.self.value_proj.bias True\n",
            "deberta.encoder.layer.23.attention.output.dense.weight True\n",
            "deberta.encoder.layer.23.attention.output.dense.bias True\n",
            "deberta.encoder.layer.23.attention.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.23.attention.output.LayerNorm.bias True\n",
            "deberta.encoder.layer.23.intermediate.dense.weight True\n",
            "deberta.encoder.layer.23.intermediate.dense.bias True\n",
            "deberta.encoder.layer.23.output.dense.weight True\n",
            "deberta.encoder.layer.23.output.dense.bias True\n",
            "deberta.encoder.layer.23.output.LayerNorm.weight True\n",
            "deberta.encoder.layer.23.output.LayerNorm.bias True\n",
            "deberta.encoder.rel_embeddings.weight True\n",
            "deberta.encoder.LayerNorm.weight True\n",
            "deberta.encoder.LayerNorm.bias True\n",
            "pooler.dense.weight True\n",
            "pooler.dense.bias True\n",
            "classifier.weight True\n",
            "classifier.bias True\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6065' max='6065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6065/6065 57:24, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Content Rmse</th>\n",
              "      <th>Wording Rmse</th>\n",
              "      <th>Mcrmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.804601</td>\n",
              "      <td>0.676987</td>\n",
              "      <td>1.001172</td>\n",
              "      <td>0.839080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.793156</td>\n",
              "      <td>0.606983</td>\n",
              "      <td>1.026703</td>\n",
              "      <td>0.816843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.735604</td>\n",
              "      <td>0.579552</td>\n",
              "      <td>0.935168</td>\n",
              "      <td>0.757360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.780764</td>\n",
              "      <td>0.683488</td>\n",
              "      <td>0.936910</td>\n",
              "      <td>0.810199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.582400</td>\n",
              "      <td>0.608297</td>\n",
              "      <td>0.554995</td>\n",
              "      <td>0.725414</td>\n",
              "      <td>0.640204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.582400</td>\n",
              "      <td>0.637870</td>\n",
              "      <td>0.619111</td>\n",
              "      <td>0.726524</td>\n",
              "      <td>0.672818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.582400</td>\n",
              "      <td>0.857584</td>\n",
              "      <td>0.798801</td>\n",
              "      <td>0.961055</td>\n",
              "      <td>0.879928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.582400</td>\n",
              "      <td>0.687353</td>\n",
              "      <td>0.649398</td>\n",
              "      <td>0.774096</td>\n",
              "      <td>0.711747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.582400</td>\n",
              "      <td>0.693795</td>\n",
              "      <td>0.630500</td>\n",
              "      <td>0.804809</td>\n",
              "      <td>0.717655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.501700</td>\n",
              "      <td>0.572538</td>\n",
              "      <td>0.486484</td>\n",
              "      <td>0.710456</td>\n",
              "      <td>0.598470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.501700</td>\n",
              "      <td>0.584682</td>\n",
              "      <td>0.499202</td>\n",
              "      <td>0.718204</td>\n",
              "      <td>0.608703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.501700</td>\n",
              "      <td>0.715688</td>\n",
              "      <td>0.579296</td>\n",
              "      <td>0.892534</td>\n",
              "      <td>0.735915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.501700</td>\n",
              "      <td>0.578848</td>\n",
              "      <td>0.511250</td>\n",
              "      <td>0.693268</td>\n",
              "      <td>0.602259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.501700</td>\n",
              "      <td>0.695508</td>\n",
              "      <td>0.655710</td>\n",
              "      <td>0.781036</td>\n",
              "      <td>0.718373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.433600</td>\n",
              "      <td>0.606319</td>\n",
              "      <td>0.558169</td>\n",
              "      <td>0.710498</td>\n",
              "      <td>0.634333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.433600</td>\n",
              "      <td>0.621982</td>\n",
              "      <td>0.565648</td>\n",
              "      <td>0.733861</td>\n",
              "      <td>0.649755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.433600</td>\n",
              "      <td>0.676583</td>\n",
              "      <td>0.629088</td>\n",
              "      <td>0.772812</td>\n",
              "      <td>0.700950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.433600</td>\n",
              "      <td>0.613782</td>\n",
              "      <td>0.572000</td>\n",
              "      <td>0.709011</td>\n",
              "      <td>0.640505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.433600</td>\n",
              "      <td>0.593390</td>\n",
              "      <td>0.527277</td>\n",
              "      <td>0.715370</td>\n",
              "      <td>0.621323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.400200</td>\n",
              "      <td>0.606623</td>\n",
              "      <td>0.557941</td>\n",
              "      <td>0.710906</td>\n",
              "      <td>0.634424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.400200</td>\n",
              "      <td>0.621364</td>\n",
              "      <td>0.610345</td>\n",
              "      <td>0.686324</td>\n",
              "      <td>0.648335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.400200</td>\n",
              "      <td>0.577028</td>\n",
              "      <td>0.516506</td>\n",
              "      <td>0.690663</td>\n",
              "      <td>0.603584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.400200</td>\n",
              "      <td>0.678965</td>\n",
              "      <td>0.664485</td>\n",
              "      <td>0.732571</td>\n",
              "      <td>0.698528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.400200</td>\n",
              "      <td>0.641944</td>\n",
              "      <td>0.615317</td>\n",
              "      <td>0.721719</td>\n",
              "      <td>0.668518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.399000</td>\n",
              "      <td>0.645812</td>\n",
              "      <td>0.664801</td>\n",
              "      <td>0.677701</td>\n",
              "      <td>0.671251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.399000</td>\n",
              "      <td>0.617272</td>\n",
              "      <td>0.585446</td>\n",
              "      <td>0.701827</td>\n",
              "      <td>0.643636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.399000</td>\n",
              "      <td>0.574912</td>\n",
              "      <td>0.514588</td>\n",
              "      <td>0.697277</td>\n",
              "      <td>0.605932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.399000</td>\n",
              "      <td>0.664424</td>\n",
              "      <td>0.657729</td>\n",
              "      <td>0.721248</td>\n",
              "      <td>0.689489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.399000</td>\n",
              "      <td>0.700896</td>\n",
              "      <td>0.735310</td>\n",
              "      <td>0.720303</td>\n",
              "      <td>0.727806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.313900</td>\n",
              "      <td>0.615847</td>\n",
              "      <td>0.572809</td>\n",
              "      <td>0.710539</td>\n",
              "      <td>0.641674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.313900</td>\n",
              "      <td>0.661388</td>\n",
              "      <td>0.648678</td>\n",
              "      <td>0.728037</td>\n",
              "      <td>0.688358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.313900</td>\n",
              "      <td>0.637154</td>\n",
              "      <td>0.598790</td>\n",
              "      <td>0.720875</td>\n",
              "      <td>0.659832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.313900</td>\n",
              "      <td>0.643389</td>\n",
              "      <td>0.639835</td>\n",
              "      <td>0.698520</td>\n",
              "      <td>0.669178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.313900</td>\n",
              "      <td>0.707123</td>\n",
              "      <td>0.704462</td>\n",
              "      <td>0.761976</td>\n",
              "      <td>0.733219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.332800</td>\n",
              "      <td>0.563964</td>\n",
              "      <td>0.494897</td>\n",
              "      <td>0.684119</td>\n",
              "      <td>0.589508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.332800</td>\n",
              "      <td>0.592889</td>\n",
              "      <td>0.530140</td>\n",
              "      <td>0.704603</td>\n",
              "      <td>0.617372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.332800</td>\n",
              "      <td>0.608718</td>\n",
              "      <td>0.576089</td>\n",
              "      <td>0.691464</td>\n",
              "      <td>0.633777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.332800</td>\n",
              "      <td>0.660602</td>\n",
              "      <td>0.667794</td>\n",
              "      <td>0.702849</td>\n",
              "      <td>0.685322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.332800</td>\n",
              "      <td>0.632416</td>\n",
              "      <td>0.631709</td>\n",
              "      <td>0.683786</td>\n",
              "      <td>0.657748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.259100</td>\n",
              "      <td>0.625145</td>\n",
              "      <td>0.606060</td>\n",
              "      <td>0.692639</td>\n",
              "      <td>0.649350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.259100</td>\n",
              "      <td>0.642827</td>\n",
              "      <td>0.649096</td>\n",
              "      <td>0.690218</td>\n",
              "      <td>0.669657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.259100</td>\n",
              "      <td>0.641044</td>\n",
              "      <td>0.642331</td>\n",
              "      <td>0.687742</td>\n",
              "      <td>0.665036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.259100</td>\n",
              "      <td>0.602181</td>\n",
              "      <td>0.573501</td>\n",
              "      <td>0.680379</td>\n",
              "      <td>0.626940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.259100</td>\n",
              "      <td>0.657344</td>\n",
              "      <td>0.675295</td>\n",
              "      <td>0.687558</td>\n",
              "      <td>0.681426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.631570</td>\n",
              "      <td>0.628567</td>\n",
              "      <td>0.682466</td>\n",
              "      <td>0.655517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.616417</td>\n",
              "      <td>0.602169</td>\n",
              "      <td>0.681338</td>\n",
              "      <td>0.641754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.651670</td>\n",
              "      <td>0.659936</td>\n",
              "      <td>0.692469</td>\n",
              "      <td>0.676202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.644469</td>\n",
              "      <td>0.629971</td>\n",
              "      <td>0.709037</td>\n",
              "      <td>0.669504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.631728</td>\n",
              "      <td>0.630033</td>\n",
              "      <td>0.683314</td>\n",
              "      <td>0.656674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.208600</td>\n",
              "      <td>0.655328</td>\n",
              "      <td>0.670317</td>\n",
              "      <td>0.688031</td>\n",
              "      <td>0.679174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.208600</td>\n",
              "      <td>0.645830</td>\n",
              "      <td>0.655043</td>\n",
              "      <td>0.684321</td>\n",
              "      <td>0.669682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.208600</td>\n",
              "      <td>0.660517</td>\n",
              "      <td>0.682195</td>\n",
              "      <td>0.687495</td>\n",
              "      <td>0.684845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>0.208600</td>\n",
              "      <td>0.655883</td>\n",
              "      <td>0.670860</td>\n",
              "      <td>0.687867</td>\n",
              "      <td>0.679364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>0.208600</td>\n",
              "      <td>0.637317</td>\n",
              "      <td>0.645344</td>\n",
              "      <td>0.679884</td>\n",
              "      <td>0.662614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.156000</td>\n",
              "      <td>0.656905</td>\n",
              "      <td>0.682975</td>\n",
              "      <td>0.679369</td>\n",
              "      <td>0.681172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.156000</td>\n",
              "      <td>0.632715</td>\n",
              "      <td>0.638516</td>\n",
              "      <td>0.677260</td>\n",
              "      <td>0.657888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>0.156000</td>\n",
              "      <td>0.633016</td>\n",
              "      <td>0.635171</td>\n",
              "      <td>0.680320</td>\n",
              "      <td>0.657745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>0.156000</td>\n",
              "      <td>0.631080</td>\n",
              "      <td>0.632750</td>\n",
              "      <td>0.679893</td>\n",
              "      <td>0.656322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>0.156000</td>\n",
              "      <td>0.639242</td>\n",
              "      <td>0.649083</td>\n",
              "      <td>0.679351</td>\n",
              "      <td>0.664217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.155300</td>\n",
              "      <td>0.639934</td>\n",
              "      <td>0.649724</td>\n",
              "      <td>0.679543</td>\n",
              "      <td>0.664633</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold 0:\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold 1:\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold 2:\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold 3:\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cv content rmse: 0.46173521257928285\n",
            "cv wording rmse: 0.6468330711904262\n"
          ]
        }
      ],
      "source": [
        "targets = [\"wording\", \"content\"]\n",
        "mode = \"multi\"\n",
        "input_cols = [\"prompt_title\", \"prompt_question\", \"text\"]\n",
        "model_cfg = CFG\n",
        "\n",
        "train_by_fold(\n",
        "    train,\n",
        "    model_name=model_cfg.model_name,\n",
        "    dir_model=model_cfg.dir_model,\n",
        "    save_each_model=False,\n",
        "    targets=targets,\n",
        "    inputs=input_cols,\n",
        "    learning_rate=model_cfg.learning_rate,\n",
        "    hidden_dropout_prob=model_cfg.hidden_dropout_prob,\n",
        "    attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,\n",
        "    weight_decay=model_cfg.weight_decay,\n",
        "    num_train_epochs=model_cfg.num_train_epochs,\n",
        "    n_splits=CFG.n_splits,\n",
        "    batch_size=model_cfg.batch_size,\n",
        "    save_steps=model_cfg.save_steps,\n",
        "    max_length=model_cfg.max_length\n",
        ")\n",
        "\n",
        "if CFG.augmentations:\n",
        "    train = train[train['augmentation']=='no']\n",
        "    train = train.drop('augmentation',axis=1)\n",
        "\n",
        "train = validate(\n",
        "    train,\n",
        "    mode=mode,\n",
        "    targets=targets,\n",
        "    inputs=input_cols,\n",
        "    save_each_model=False,\n",
        "    n_splits=CFG.n_splits,\n",
        "    batch_size=model_cfg.batch_size,\n",
        "    model_name=model_cfg.model_name,\n",
        "    dir_model=model_cfg.dir_model,\n",
        "    hidden_dropout_prob=model_cfg.hidden_dropout_prob,\n",
        "    attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,\n",
        "    max_length=model_cfg.max_length\n",
        ")\n",
        "\n",
        "# set validate result\n",
        "for target in [\"content\", \"wording\"]:\n",
        "    rmse = mean_squared_error(train[target], train[f\"{target}_{mode}_pred\"], squared=False)\n",
        "    print(f\"cv {target} rmse: {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6mW9zP-maUl"
      },
      "source": [
        "###Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5CJT1eDemb_G",
        "outputId": "d48be1eb-abfb-454d-c060-4b24513e017f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4de74817-f199-4d4b-87a2-61d0ff50df48\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>student_id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>text</th>\n",
              "      <th>content</th>\n",
              "      <th>wording</th>\n",
              "      <th>summary_length</th>\n",
              "      <th>splling_err_num</th>\n",
              "      <th>prompt_question</th>\n",
              "      <th>prompt_title</th>\n",
              "      <th>prompt_text</th>\n",
              "      <th>...</th>\n",
              "      <th>text_standard</th>\n",
              "      <th>spache_readability</th>\n",
              "      <th>mcalpine_eflaw</th>\n",
              "      <th>reading_time</th>\n",
              "      <th>syllable_count</th>\n",
              "      <th>polysyllabcount</th>\n",
              "      <th>monosyllabcount</th>\n",
              "      <th>fold</th>\n",
              "      <th>content_multi_pred</th>\n",
              "      <th>wording_multi_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000e8c3c7ddb</td>\n",
              "      <td>814d6b</td>\n",
              "      <td>The third wave was an experimentto see how peo...</td>\n",
              "      <td>0.205683</td>\n",
              "      <td>0.380538</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>Summarize how the Third Wave developed over su...</td>\n",
              "      <td>The Third Wave</td>\n",
              "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.54</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.20</td>\n",
              "      <td>93</td>\n",
              "      <td>7</td>\n",
              "      <td>40</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.060547</td>\n",
              "      <td>0.817871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0020ae56ffbf</td>\n",
              "      <td>ebad26</td>\n",
              "      <td>They would rub it up with soda to make the sme...</td>\n",
              "      <td>-0.548304</td>\n",
              "      <td>0.506755</td>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>Summarize the various ways the factory would u...</td>\n",
              "      <td>Excerpt from The Jungle</td>\n",
              "      <td>With one member trimming beef in a cannery, an...</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>38.5</td>\n",
              "      <td>2.84</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.770508</td>\n",
              "      <td>-0.442871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>004e978e639e</td>\n",
              "      <td>3b9047</td>\n",
              "      <td>In Egypt, there were many occupations and soci...</td>\n",
              "      <td>3.128928</td>\n",
              "      <td>4.231226</td>\n",
              "      <td>275</td>\n",
              "      <td>3</td>\n",
              "      <td>In complete sentences, summarize the structure...</td>\n",
              "      <td>Egyptian Social Structure</td>\n",
              "      <td>Egyptian society was structured like a pyramid...</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.95</td>\n",
              "      <td>26.8</td>\n",
              "      <td>16.69</td>\n",
              "      <td>317</td>\n",
              "      <td>14</td>\n",
              "      <td>170</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.558594</td>\n",
              "      <td>2.748047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>005ab0199905</td>\n",
              "      <td>3b9047</td>\n",
              "      <td>The highest class was Pharaohs these people we...</td>\n",
              "      <td>-0.210614</td>\n",
              "      <td>-0.471415</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>In complete sentences, summarize the structure...</td>\n",
              "      <td>Egyptian Social Structure</td>\n",
              "      <td>Egyptian society was structured like a pyramid...</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.39</td>\n",
              "      <td>11.3</td>\n",
              "      <td>1.95</td>\n",
              "      <td>37</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.699219</td>\n",
              "      <td>-0.434326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0070c9e7af47</td>\n",
              "      <td>814d6b</td>\n",
              "      <td>The Third Wave developed  rapidly because the ...</td>\n",
              "      <td>3.272894</td>\n",
              "      <td>3.219757</td>\n",
              "      <td>236</td>\n",
              "      <td>15</td>\n",
              "      <td>Summarize how the Third Wave developed over su...</td>\n",
              "      <td>The Third Wave</td>\n",
              "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
              "      <td>...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.32</td>\n",
              "      <td>20.2</td>\n",
              "      <td>14.98</td>\n",
              "      <td>301</td>\n",
              "      <td>21</td>\n",
              "      <td>136</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.975586</td>\n",
              "      <td>2.103516</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 54 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4de74817-f199-4d4b-87a2-61d0ff50df48')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4de74817-f199-4d4b-87a2-61d0ff50df48 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4de74817-f199-4d4b-87a2-61d0ff50df48');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-27514f41-2c4f-470d-8f50-9de1ebada03c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-27514f41-2c4f-470d-8f50-9de1ebada03c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-27514f41-2c4f-470d-8f50-9de1ebada03c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     student_id prompt_id                                               text  \\\n",
              "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
              "1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
              "2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n",
              "3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n",
              "4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
              "\n",
              "    content   wording  summary_length  splling_err_num  \\\n",
              "0  0.205683  0.380538              64                2   \n",
              "1 -0.548304  0.506755              55                1   \n",
              "2  3.128928  4.231226             275                3   \n",
              "3 -0.210614 -0.471415              32                3   \n",
              "4  3.272894  3.219757             236               15   \n",
              "\n",
              "                                     prompt_question  \\\n",
              "0  Summarize how the Third Wave developed over su...   \n",
              "1  Summarize the various ways the factory would u...   \n",
              "2  In complete sentences, summarize the structure...   \n",
              "3  In complete sentences, summarize the structure...   \n",
              "4  Summarize how the Third Wave developed over su...   \n",
              "\n",
              "                prompt_title  \\\n",
              "0             The Third Wave   \n",
              "1    Excerpt from The Jungle   \n",
              "2  Egyptian Social Structure   \n",
              "3  Egyptian Social Structure   \n",
              "4             The Third Wave   \n",
              "\n",
              "                                         prompt_text  ...  text_standard  \\\n",
              "0  Background \\r\\nThe Third Wave experiment took ...  ...            8.0   \n",
              "1  With one member trimming beef in a cannery, an...  ...            8.0   \n",
              "2  Egyptian society was structured like a pyramid...  ...            9.0   \n",
              "3  Egyptian society was structured like a pyramid...  ...            5.0   \n",
              "4  Background \\r\\nThe Third Wave experiment took ...  ...           10.0   \n",
              "\n",
              "   spache_readability  mcalpine_eflaw  reading_time  syllable_count  \\\n",
              "0                4.54            22.0          4.20              93   \n",
              "1                5.00            38.5          2.84              56   \n",
              "2                4.95            26.8         16.69             317   \n",
              "3                3.39            11.3          1.95              37   \n",
              "4                4.32            20.2         14.98             301   \n",
              "\n",
              "   polysyllabcount  monosyllabcount  fold  content_multi_pred  \\\n",
              "0                7               40   3.0            0.060547   \n",
              "1                0               48   2.0           -0.770508   \n",
              "2               14              170   1.0            2.558594   \n",
              "3                4               18   1.0           -0.699219   \n",
              "4               21              136   3.0            1.975586   \n",
              "\n",
              "   wording_multi_pred  \n",
              "0            0.817871  \n",
              "1           -0.442871  \n",
              "2            2.748047  \n",
              "3           -0.434326  \n",
              "4            2.103516  \n",
              "\n",
              "[5 rows x 54 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.to_csv(CFG.save_model_path+f\"/train-predictions_{EXP_NUM}.csv\",index=False)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T4FJ5_qm3Uql"
      },
      "outputs": [],
      "source": [
        "os.makedirs(f\"/content/drive/MyDrive/CommonLit/{CFG.model_name}\",exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DFZSVbzUg1L2"
      },
      "outputs": [],
      "source": [
        "train.to_csv(f\"/content/drive/MyDrive/CommonLit/{CFG.model_name}/train-predictions_{EXP_NUM}.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_ZsIwxSdvTeQ"
      },
      "outputs": [],
      "source": [
        "f = open(f\"/content/drive/MyDrive/CommonLit/{CFG.model_name}/results_{EXP_NUM}.txt\", \"w\")\n",
        "\n",
        "for target in [\"content\", \"wording\"]:\n",
        "    rmse = mean_squared_error(train[target], train[f\"{target}_{mode}_pred\"], squared=False)\n",
        "    f.write(f\"cv {target} rmse: {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DybwNrt8kqtO"
      },
      "source": [
        "###Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t95KMidqknhF",
        "outputId": "f3d6ebc1-4a64-4e6e-bd1e-d3ec4b432366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/bert-large-cased-model/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/trainer_state.json (deflated 76%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/config.json (deflated 52%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/rng_state.pth (deflated 28%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/pytorch_model.bin (deflated 7%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/vocab.txt (deflated 49%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/training_args.bin (deflated 48%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/tokenizer.json (deflated 70%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/optimizer.pt (deflated 9%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/added_tokens.json (deflated 37%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/scheduler.pt (deflated 49%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_3/3/checkpoint-1800/tokenizer_config.json (deflated 75%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/trainer_state.json (deflated 75%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/config.json (deflated 52%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/rng_state.pth (deflated 28%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/pytorch_model.bin (deflated 7%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/vocab.txt (deflated 49%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/training_args.bin (deflated 48%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/tokenizer.json (deflated 70%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/optimizer.pt (deflated 8%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/added_tokens.json (deflated 37%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/scheduler.pt (deflated 49%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_1/1/checkpoint-1400/tokenizer_config.json (deflated 75%)\n",
            "  adding: content/bert-large-cased-model/exp_1/train-predictions_1.csv (deflated 88%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/trainer_state.json (deflated 69%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/config.json (deflated 52%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/rng_state.pth (deflated 28%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/pytorch_model.bin (deflated 7%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/vocab.txt (deflated 49%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/training_args.bin (deflated 48%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/tokenizer.json (deflated 70%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/optimizer.pt (deflated 8%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/added_tokens.json (deflated 37%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/scheduler.pt (deflated 49%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_2/2/checkpoint-500/tokenizer_config.json (deflated 75%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/ (stored 0%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/trainer_state.json (deflated 73%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/config.json (deflated 52%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/rng_state.pth (deflated 28%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/pytorch_model.bin (deflated 7%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/vocab.txt (deflated 49%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/training_args.bin (deflated 48%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/tokenizer.json (deflated 70%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/optimizer.pt (deflated 8%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/added_tokens.json (deflated 37%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/scheduler.pt (deflated 49%)\n",
            "  adding: content/bert-large-cased-model/exp_1/fold_0/0/checkpoint-900/tokenizer_config.json (deflated 75%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/drive/MyDrive/CommonLit/deberta-v3-base/exp_3.zip  /content/deberta-v3-base-model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPb7enCYmclz"
      },
      "source": [
        "###LGBM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zh3jWeczmgk4"
      },
      "outputs": [],
      "source": [
        "targets = [\"content\", \"wording\"]\n",
        "\n",
        "drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\",\n",
        "                \"prompt_question\", \"prompt_title\",\n",
        "                \"prompt_text\",\"prompt_length\",\n",
        "                \"avg_word_length\",\"semicolon_count\",\"neg\",\"neu\",\"pos\",\"compound\",\n",
        "                \"exclamation_count\",\"question_count\",\"punctuation_sum\",\"neg_prompt\",\"neu_prompt\",\"pos_prompt\",\n",
        "                \"compound_prompt\",\"flesch_reading_ease\",\"flesch_kincaid_grade\",\"gunning_fog\",\"automated_readability_index\",\n",
        "                \"coleman_liau_index\",\"linsear_write_formula\",\"dale_chall_readability_score\",\"text_standard\",\"spache_readability\",\n",
        "                \"mcalpine_eflaw\"\n",
        "               ] + targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ry3t32oLmje6",
        "outputId": "2f6cfc16-e882-4787-d0d8-a387f5869cae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001145 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3004\n",
            "[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 21\n",
            "[LightGBM] [Info] Start training from score 0.017606\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "[100]\ttrain's rmse: 0.412134\n",
            "Early stopping, best iteration is:\n",
            "[70]\ttrain's rmse: 0.410412\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2926\n",
            "[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 21\n",
            "[LightGBM] [Info] Start training from score -0.039959\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttrain's rmse: 0.529247\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000630 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2927\n",
            "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 21\n",
            "[LightGBM] [Info] Start training from score 0.013356\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "[100]\ttrain's rmse: 0.415136\n",
            "Early stopping, best iteration is:\n",
            "[71]\ttrain's rmse: 0.411327\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3022\n",
            "[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 21\n",
            "[LightGBM] [Info] Start training from score -0.044904\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "[100]\ttrain's rmse: 0.46378\n",
            "Early stopping, best iteration is:\n",
            "[98]\ttrain's rmse: 0.463671\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000599 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3004\n",
            "[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 21\n",
            "[LightGBM] [Info] Start training from score -0.031791\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[69]\ttrain's rmse: 0.52356\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000518 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2926\n",
            "[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 21\n",
            "[LightGBM] [Info] Start training from score -0.060941\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[20]\ttrain's rmse: 0.69878\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2927\n",
            "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 21\n",
            "[LightGBM] [Info] Start training from score 0.028040\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "[100]\ttrain's rmse: 0.505621\n",
            "Early stopping, best iteration is:\n",
            "[114]\ttrain's rmse: 0.504887\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3022\n",
            "[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 21\n",
            "[LightGBM] [Info] Start training from score -0.168933\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "[100]\ttrain's rmse: 0.669436\n",
            "Early stopping, best iteration is:\n",
            "[149]\ttrain's rmse: 0.665208\n"
          ]
        }
      ],
      "source": [
        "model_dict = {}\n",
        "\n",
        "for target in targets:\n",
        "    models = []\n",
        "\n",
        "    for fold in range(CFG.n_splits):\n",
        "\n",
        "        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n",
        "        y_train_cv = train[train[\"fold\"] != fold][target]\n",
        "\n",
        "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
        "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
        "        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n",
        "\n",
        "        params = {\n",
        "                  'boosting_type': 'gbdt',\n",
        "                  'random_state': 42,\n",
        "                  'objective': 'regression',\n",
        "                  'metric': 'rmse',\n",
        "                  'learning_rate': 0.048,\n",
        "                  'lambda_l1': 0.0,\n",
        "                  'lambda_l2': 0.011\n",
        "                  }\n",
        "\n",
        "        evaluation_results = {}\n",
        "        model = lgb.train(params,\n",
        "                          num_boost_round=10000,\n",
        "                            #categorical_feature = categorical_features,\n",
        "                          valid_names=['train', 'valid'],\n",
        "                          train_set=dtrain,\n",
        "                          valid_sets=dval,\n",
        "                          callbacks=[\n",
        "                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n",
        "                               lgb.log_evaluation(100),\n",
        "                              lgb.callback.record_evaluation(evaluation_results)\n",
        "                            ],\n",
        "                          )\n",
        "        models.append(model)\n",
        "\n",
        "    model_dict[target] = models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p23s2kfKmnKc"
      },
      "source": [
        "### CV Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7vMypbdJmoSH",
        "outputId": "ceb7f509-7314-4e61-db5b-a4db11a8138d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content_rmse : 0.4804265147960582\n",
            "wording_rmse : 0.6033187479833971\n",
            "mcrmse : 0.5418726313897276\n"
          ]
        }
      ],
      "source": [
        "# cv\n",
        "rmses = []\n",
        "\n",
        "for target in targets:\n",
        "    models = model_dict[target]\n",
        "\n",
        "    preds = []\n",
        "    trues = []\n",
        "\n",
        "    for fold, model in enumerate(models):\n",
        "        # ilocで取り出す行を指定\n",
        "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
        "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
        "\n",
        "        pred = model.predict(X_eval_cv)\n",
        "\n",
        "        trues.extend(y_eval_cv)\n",
        "        preds.extend(pred)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
        "    print(f\"{target}_rmse : {rmse}\")\n",
        "    f.write(f\"{target}_rmse : {rmse}\")\n",
        "    rmses = rmses + [rmse]\n",
        "\n",
        "print(f\"mcrmse : {sum(rmses) / len(rmses)}\")\n",
        "f.write(f\"mcrmse : {sum(rmses) / len(rmses)}\")\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uA6U-VyZMXdm",
        "BYTTUlxgMaeE",
        "pn8aSZlEMq3-",
        "vEW4wqN7MlYx",
        "3ctfO27vM7p3",
        "Hwx5izVXNLl6",
        "a8g7AgjlPhGS",
        "h8id7693RhaR",
        "eCMOX6R8Rnwf",
        "M3z0L4BvRxN-",
        "pZNPjFyvR-r4",
        "cPma9z5BTNDe"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}